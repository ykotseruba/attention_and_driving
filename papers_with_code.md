---
<a href=README.md/#top><l style="font-size:30px">Home</l></a>&nbsp;&nbsp;| <a href=behavioral.md><l style="font-size:30px">Behavioral</l></a>&nbsp;&nbsp;| <l style="font-size:35px">Applications</l>&nbsp;&nbsp;| <a href=datasets.md><l style="font-size:30px">Datasets</l></a>&nbsp;&nbsp;
---

[Scene gaze](scene_gaze.md)&nbsp;&nbsp;| [In-vehicle gaze](in-vehicle_gaze.md)&nbsp;&nbsp;| [Distraction detection](distraction_detection.md)&nbsp;&nbsp;| [Drowsiness detection](drowsiness_detection.md)&nbsp;&nbsp;| [Action anticipation](action_anticipation.md)&nbsp;&nbsp;| [Driver awareness](driver_awareness.md)&nbsp;&nbsp;| [Self-driving](self-driving.md)&nbsp;&nbsp;| Papers with code&nbsp;&nbsp;
___
*Click on each entry below to see additional information.*

### Action anticipation

<ul><a name=2022_IROS_Peng></a>
<details close>
<summary>Peng et al., TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration, IROS, 2022 | <a href=https://doi.org/10.1109/IROS47612.2022.9981445>paper</a> | <a href=https://github.com/KPeng9510/TransDARC>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#Drive&Act>Drive&Act</a>
</ul>
<ul>
<pre>
@inproceedings{2022_IROS_Peng,
    author = "Peng, Kunyu and Roitberg, Alina and Yang, Kailun and Zhang, Jiaming and Stiefelhagen, Rainer",
    booktitle = "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
    organization = "IEEE",
    pages = "278--285",
    title = "TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration",
    year = "2022"
}
</pre>
</ul>
</ul>
<ul><a name=2016_ICRA_Jain></a>
<details close>
<summary>Jain et al., Recurrent Neural Networks for Driver Activity Anticipation via Sensory-Fusion Architecture, ICRA, 2016 | <a href=https://doi.org/10.1109/ICRA.2016.7487478>paper</a> | <a href=https://github.com/asheshjain399/RNNexp>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#Brain4Cars>Brain4Cars</a>
</ul>
<ul>
<pre>
@inproceedings{2016_ICRA_Jain,
    author = "Jain, Ashesh and Singh, Avi and Koppula, Hema S and Soh, Shane and Saxena, Ashutosh",
    booktitle = "ICRA",
    title = "Recurrent neural networks for driver activity anticipation via sensory-fusion architecture",
    year = "2016"
}
</pre>
</ul>
</ul>

### Scene gaze

<ul><a name=2022_T-ITS_Li></a>
<details close>
<summary>Li et al., Adaptive Short-Temporal Induced Aware Fusion Network for Predicting Attention Regions Like a Driver, Trans. ITS, 2022 | <a href=https://doi.org/10.1109/TITS.2022.3165619>paper</a> | <a href=https://github.com/liuchunsense/ASIAFnet>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-A>BDD-A</a>, <a href=datasets.md#DADA-2000>DADA-2000</a>, <a href=datasets.md#TrafficSaliency>TrafficSaliency</a>
</ul>
<ul>
<pre>
@article{2022_T-ITS_Li,
    author = "Li, Qiang and Liu, Chunsheng and Chang, Faliang and Li, Shuang and Liu, Hui and Liu, Zehao",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "10",
    pages = "18695--18706",
    publisher = "IEEE",
    title = "Adaptive short-temporal induced aware fusion network for predicting attention regions like a driver",
    volume = "23",
    year = "2022"
}
</pre>
</ul>
</ul>
<ul><a name=2022_IV_Araluce></a>
<details close>
<summary>Araluce et al., ARAGAN: A dRiver Attention estimation model based on conditional Generative Adversarial Network, IV, 2022 | <a href=https://doi.org/10.1109/IV51971.2022.9827175>paper</a> | <a href=https://github.com/javierAraluce/ARAGAN>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-A>BDD-A</a>, <a href=datasets.md#DADA-2000>DADA-2000</a>
</ul>
<ul>
<pre>
@inproceedings{2022_IV_Araluce,
    author = "Araluce, Javier and Bergasa, Luis M and Oca{\\textasciitilde n}a, Manuel and Barea, Rafael and L{\'o}pez-Guill{\'e}n, Elena and Revenga, Pedro",
    booktitle = "2022 IEEE Intelligent Vehicles Symposium (IV)",
    organization = "IEEE",
    pages = "1066--1072",
    title = "ARAGAN: A dRiver Attention estimation model based on conditional Generative Adversarial Network",
    year = "2022"
}
</pre>
</ul>
</ul>
<ul><a name=2022_ECCV_Kasahara></a>
<details close>
<summary>Kasahara et al., Look Both Ways: Self-Supervising Driver Gaze Estimation and Road Scene Saliency, ECCV, 2022 | <a href=https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136730128.pdf>paper</a> | <a href=https://github.com/Kasai2020/look_both_ways>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#LBW>LBW</a>
</ul>
<ul>
<pre>
@inproceedings{2022_ECCV_Kasahara,
    author = "Kasahara, Isaac and Stent, Simon and Park, Hyun Soo",
    booktitle = "Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XIII",
    organization = "Springer",
    pages = "126--142",
    title = "Look Both Ways: Self-supervising Driver Gaze Estimation and Road Scene Saliency",
    year = "2022"
}
</pre>
</ul>
</ul>
<ul><a name=2022_T-ITS_Fang></a>
<details close>
<summary>Fang et al., DADA: Driver Attention Prediction in Driving Accident Scenarios, Trans. ITS, 2021 | <a href=https://doi.org/10.1109/TITS.2020.3044678>paper</a> | <a href=https://github.com/JWFangit/LOTVS-DADA>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#TrafficSaliency>TrafficSaliency</a>, <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>, <a href=datasets.md#DADA-2000>DADA-2000</a>
</ul>
<ul>
<pre>
@article{2022_T-ITS_Fang,
    author = "Fang, Jianwu and Yan, Dingxin and Qiao, Jiahuan and Xue, Jianru and Yu, Hongkai",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "6",
    pages = "4959--4971",
    publisher = "IEEE",
    title = "DADA: Driver attention prediction in driving accident scenarios",
    volume = "23",
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2021_ICCVW_Gopinath></a>
<details close>
<summary>Gopinath et al., MAAD: A Model and Dataset for “Attended Awareness” in Driving, ICCVW, 2021 | <a href=https://openaccess.thecvf.com/content/ICCV2021W/EPIC/papers/Gopinath_MAAD_A_Model_and_Dataset_for_Attended_Awareness_in_Driving_ICCVW_2021_paper.pdf>paper</a> | <a href=https://github.com/ToyotaResearchInstitute/att-aware/>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#MAAD>MAAD</a>
</ul>
<ul>
<pre>
@inproceedings{2021_ICCVW_Gopinath,
    author = "Gopinath, Deepak and Rosman, Guy and Stent, Simon and Terahata, Katsuya and Fletcher, Luke and Argall, Brenna and Leonard, John",
    booktitle = "Proceedings of the IEEE/CVF International Conference on Computer Vision",
    pages = "3426--3436",
    title = {MAAD: A Model and Dataset for" Attended Awareness" in Driving},
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2021_ICCV_Baee></a>
<details close>
<summary>Baee et al., MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning, ICCV, 2021 | <a href=https://openaccess.thecvf.com/content/ICCV2021/papers/Baee_MEDIRL_Predicting_the_Visual_Attention_of_Drivers_via_Maximum_Entropy_ICCV_2021_paper.pdf>paper</a> | <a href=https://github.com/soniabaee/MEDIRL-EyeCar>code</a></summary>
<ul>
Dataset(s): Eyecar
</ul>
<ul>
<pre>
@inproceedings{2021_ICCV_Baee,
    author = "Baee, Sonia and Pakdamanian, Erfan and Kim, Inki and Feng, Lu and Ordonez, Vicente and Barnes, Laura",
    booktitle = "ICCV",
    title = "MEDIRL: Predicting the visual attention of drivers via maximum entropy deep inverse reinforcement learning",
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2020_T-ITS_Deng></a>
<details close>
<summary>Deng et al., How Do Drivers Allocate Their Potential Attention? Driving Fixation Prediction via Convolutional Neural Networks, Trans. ITS, 2020 | <a href=https://doi.org/10.1109/TITS.2019.2915540>paper</a> | <a href=https://github.com/taodeng/CDNN-traffic-saliency>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#TrafficSaliency>TrafficSaliency</a>
</ul>
<ul>
<pre>
@article{2020_T-ITS_Deng,
    author = "Deng, Tao and Yan, Hongmei and Qin, Long and Ngo, Thuyen and Manjunath, BS",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "5",
    pages = "2146--2154",
    publisher = "IEEE",
    title = "{How do drivers allocate their potential attention? Driving fixation prediction via convolutional neural networks}",
    volume = "21",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2020_CVPR_Pal></a>
<details close>
<summary>Pal et al., “Looking at the right stuff” - Guided semantic-gaze for autonomous driving, CVPR, 2020 | <a href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Pal_Looking_at_the_Right_Stuff_-_Guided_Semantic-Gaze_for_Autonomous_CVPR_2020_paper.pdf>paper</a> | <a href=https://sites.google.com/eng.ucsd.edu/sage-net>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>, <a href=datasets.md#BDD-A>BDD-A</a>, JAAD
</ul>
<ul>
<pre>
@inproceedings{2020_CVPR_Pal,
    author = "Pal, Anwesan and Mondal, Sayan and Christensen, Henrik I",
    booktitle = "CVPR",
    title = {{" Looking at the Right Stuff"-Guided Semantic-Gaze for Autonomous Driving}},
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2018_PAMI_Palazzi></a>
<details close>
<summary>Palazzi et al., Predicting the Driver’s Focus of Attention: the DR(eye)VE Project, PAMI, 2018 | <a href=https://doi.org/10.1109/TPAMI.2018.2845370>paper</a> | <a href=https://github.com/ndrplz/dreyeve>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>
</ul>
<ul>
<pre>
@article{2018_PAMI_Palazzi,
    author = "Palazzi, Andrea and Abati, Davide and Solera, Francesco and Cucchiara, Rita and others",
    journal = "IEEE TPAMI",
    number = "7",
    pages = "1720--1733",
    title = "{Predicting the Driver's Focus of Attention: the DR (eye) VE Project}",
    volume = "41",
    year = "2018"
}
</pre>
</ul>
</ul>
<ul><a name=2018_ACCV_Xia></a>
<details close>
<summary>Xia et al., Predicting Driver Attention in Critical Situations, ACCV, 2018 | <a href=https://doi.org/10.1007/978-3-030-20873-8_42>paper</a> | <a href=https://github.com/pascalxia/driver_attention_prediction>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-A>BDD-A</a>
</ul>
<ul>
<pre>
@inproceedings{2018_ACCV_Xia,
    author = "Xia, Ye and Zhang, Danqing and Kim, Jinkyu and Nakayama, Ken and Zipser, Karl and Whitney, David",
    booktitle = "ACCV",
    title = "Predicting driver attention in critical situations",
    year = "2018"
}
</pre>
</ul>
</ul>
<ul><a name=2017_PR_Ohn-Bar></a>
<details close>
<summary>Ohn-Bar et al., Are all objects equal? Deep spatio-temporal importance prediction in driving videos, Pattern Recognition, 2017 | <a href=https://doi.org/10.1016/j.patcog.2016.08.029>paper</a> | <a href=https://github.com/eshed1/Object_Importance>code</a></summary>
<ul>
Dataset(s): KITTI
</ul>
<ul>
<pre>
@article{2017_PR_Ohn-Bar,
    author = "Ohn-Bar, Eshed and Trivedi, Mohan Manubhai",
    journal = "Pattern Recognition",
    pages = "425--436",
    title = "Are all objects equal? Deep spatio-temporal importance prediction in driving videos",
    volume = "64",
    year = "2017"
}
</pre>
</ul>
</ul>
<ul><a name=2017_IV_Palazzi></a>
<details close>
<summary>Palazzi et al., Learning Where to Attend Like a Human Driver, IV, 2017 | <a href=https://doi.org/10.1109/IVS.2017.7995833>paper</a> | <a href=https://github.com/francescosolera/dreyeving>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>
</ul>
<ul>
<pre>
@inproceedings{2017_IV_Palazzi,
    author = "Palazzi, Andrea and Solera, Francesco and Calderara, Simone and Alletto, Stefano and Cucchiara, Rita",
    booktitle = "IV",
    title = "Learning where to attend like a human driver",
    year = "2017"
}
</pre>
</ul>
</ul>
<ul><a name=2016_T-ITS_Deng></a>
<details close>
<summary>Deng et al., Where Does the Driver Look? Top-Down-Based Saliency Detection in a Traffic Driving Environment, Trans. ITS, 2016 | <a href=https://doi.org/10.1109/TITS.2016.2535402>paper</a> | <a href=https://github.com/taodeng/Top-down-based-traffic-driving-saliency-model>code</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@article{2016_T-ITS_Deng,
    author = "Deng, Tao and Yang, Kaifu and Li, Yongjie and Yan, Hongmei",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "7",
    pages = "2051--2062",
    publisher = "IEEE",
    title = "Where does the driver look? Top-down-based saliency detection in a traffic driving environment",
    volume = "17",
    year = "2016"
}
</pre>
</ul>
</ul>
<ul><a name=2013_RSTB_Johnson></a>
<details close>
<summary>Johnson et al., Predicting human visuomotor behaviour in a driving task, Philosophical Transactions of the Royal Society: B, 2013 | <a href=https://doi.org/10.1098/rstb.2013.0044>paper</a> | <a href=https://github.com/EmbodiedCognition/driving-simulator>code</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@article{2013_RSTB_Johnson,
    author = "Johnson, Leif and Sullivan, Brian and Hayhoe, Mary and Ballard, Dana",
    journal = "Philosophical Transactions of the Royal Society B: Biological Sciences",
    number = "1636",
    pages = "20130044",
    title = "Predicting human visuomotor behaviour in a driving task",
    volume = "369",
    year = "2014"
}
</pre>
</ul>
</ul>
<ul><a name=2012_CVPR_Borji></a>
<details close>
<summary>Borji et al., Probabilistic Learning of Task-Specific Visual Attention, CVPR, 2012 | <a href=https://doi.org/10.1109/CVPR.2012.6247710>paper</a> | <a href=http://ilab.usc.edu/borji/Resources.html>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#3DDS>3DDS</a>
</ul>
<ul>
<pre>
@inproceedings{2012_CVPR_Borji,
    author = "Borji, Ali and Sihite, Dicky N and Itti, Laurent",
    booktitle = "CVPR",
    title = "Probabilistic learning of task-specific visual attention",
    year = "2012"
}
</pre>
</ul>
</ul>

### In-vehicle gaze

<ul><a name=2020_IV_Rangesh></a>
<details close>
<summary>Rangesh et al., Driver Gaze Estimation in the Real World: Overcoming the Eyeglass Challenge, IV, 2020 | <a href=https://doi.org/10.1109/IV47402.2020.9304573>paper</a> | <a href=https://github.com/arangesh/GPCycleGAN>code</a></summary>
<ul>
Dataset(s): LISA v3
</ul>
<ul>
<pre>
@inproceedings{2020_IV_Rangesh,
    author = "Rangesh, Akshay and Zhang, Bowen and Trivedi, Mohan M",
    booktitle = "IV",
    title = "Driver gaze estimation in the real world: Overcoming the eyeglass challenge",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2020_ICMI_Stappen></a>
<details close>
<summary>Stappen et al., X-AWARE: ConteXt-AWARE Human-Environment Attention Fusion for Driver Gaze Prediction in the Wild, ICMI, 2020 | <a href=https://doi.org/10.1145/3382507.3417967>paper</a> | <a href=https://github.com/lstappen/XAWARE>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#DGW>DGW</a>
</ul>
<ul>
<pre>
@inproceedings{2020_ICMI_Stappen,
    author = {Stappen, Lukas and Rizos, Georgios and Schuller, Bj{\"o}rn},
    booktitle = "ICMI",
    title = "X-AWARE: ConteXt-AWARE Human-Environment Attention Fusion for Driver Gaze Prediction in the Wild",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2020_HumanFactors_Jokinen></a>
<details close>
<summary>Jokinen et al., Multitasking in Driving as Optimal Adaptation Under Uncertainty, Human Factors, 2020 | <a href=https://doi.org/10.1177/0018720820927687>paper</a> | <a href=https://gitlab.com/jokinenj/multitasking-driving>code</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@article{2020_HumanFactors_Jokinen,
    author = "Jokinen, Jussi PP and Kujala, Tuomo and Oulasvirta, Antti",
    journal = "Human factors",
    number = "8",
    pages = "1324--1341",
    publisher = "Sage Publications Sage CA: Los Angeles, CA",
    title = "Multitasking in driving as optimal adaptation under uncertainty",
    volume = "63",
    year = "2021"
}
</pre>
</ul>
</ul>

### Driver awareness

<ul><a name=2022_T-ITS_Zhou></a>
<details close>
<summary>Zhou et al., Using Eye-Tracking Data to Predict Situation Awareness in Real Time During Takeover Transitions in Conditionally Automated Driving, Trans. ITS, 2021 | <a href=https://doi.org/10.1109/TITS.2021.3069776>paper</a> | <a href=https://github.com/refengchou/Situation-awareness-prediction>code</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@article{2022_T-ITS_Zhou,
    author = "Zhou, Feng and Yang, X Jessie and de Winter, Joost CF",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "3",
    pages = "2284--2295",
    publisher = "IEEE",
    title = "Using eye-tracking data to predict situation awareness in real time during takeover transitions in conditionally automated driving",
    volume = "23",
    year = "2021"
}
</pre>
</ul>
</ul>

### Self-driving

<ul><a name=2021_ICCV_Chitta></a>
<details close>
<summary>Chitta et al., NEAT: Neural Attention Fields for End-to-End Autonomous Driving, ICCV, 2021 | <a href=https://openaccess.thecvf.com/content/ICCV2021/papers/Chitta_NEAT_Neural_Attention_Fields_for_End-to-End_Autonomous_Driving_ICCV_2021_paper.pdf>paper</a> | <a href=https://github.com/autonomousvision/neat>code</a></summary>
<ul>
Dataset(s): CARLA
</ul>
<ul>
<pre>
@inproceedings{2021_ICCV_Chitta,
    author = "Chitta, Kashyap and Prakash, Aditya and Geiger, Andreas",
    booktitle = "ICCV",
    title = "NEAT: Neural Attention Fields for End-to-End Autonomous Driving",
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2021_CVPR_Prakash></a>
<details close>
<summary>Prakash et al., Multi-Modal Fusion Transformer for End-to-End Autonomous Driving, CVPR, 2021 | <a href=https://openaccess.thecvf.com/content/CVPR2021/papers/Prakash_Multi-Modal_Fusion_Transformer_for_End-to-End_Autonomous_Driving_CVPR_2021_paper.pdf#page=1&zoom=auto,-100,798>paper</a> | <a href=https://github.com/autonomousvision/transfuser>code</a></summary>
<ul>
Dataset(s): CARLA
</ul>
<ul>
<pre>
@inproceedings{2021_CVPR_Prakash,
    author = "Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas",
    booktitle = "CVPR",
    title = "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving",
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2020_WACV_Xia></a>
<details close>
<summary>Xia et al., Periphery-Fovea Multi-Resolution Driving Model Guided by Human Attention, WACV, 2020 | <a href=https://openaccess.thecvf.com/content_WACV_2020/papers/Xia_Periphery-Fovea_Multi-Resolution_Driving_Model_Guided_by_Human_Attention_WACV_2020_paper.pdf>paper</a> | <a href=https://github.com/pascalxia/periphery_fovea_driving>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-X>BDD-X</a>, <a href=datasets.md#BDD-A>BDD-A</a>, <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>
</ul>
<ul>
<pre>
@inproceedings{2020_WACV_Xia,
    author = "Xia, Ye and Kim, Jinkyu and Canny, John and Zipser, Karl and Canas-Bajo, Teresa and Whitney, David",
    booktitle = "WACV",
    title = "Periphery-fovea multi-resolution driving model guided by human attention",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2020_ECCVW_Mittal></a>
<details close>
<summary>Mittal et al., AttnGrounder: Talking to Cars with Attention, ECCVW, 2020 | <a href=https://doi.org/10.1007/978-3-030-66096-3_6>paper</a> | <a href=https://github.com/i-m-vivek/AttnGrounder>code</a></summary>
<ul>
Dataset(s): Talk2Car
</ul>
<ul>
<pre>
@inproceedings{2020_ECCVW_Mittal,
    author = "Mittal, Vivek",
    booktitle = "ECCV",
    title = "Attngrounder: Talking to cars with attention",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2020_CVPR_Kim></a>
<details close>
<summary>Kim et al., Advisable Learning for Self-driving Vehicles by Internalizing Observation-to-Action Rules, CVPR, 2020 | <a href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Advisable_Learning_for_Self-Driving_Vehicles_by_Internalizing_Observation-to-Action_Rules_CVPR_2020_paper.pdf>paper</a> | <a href=https://github.com/JinkyuKimUCB/advisable-driving>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-X>BDD-X</a>, CARLA
</ul>
<ul>
<pre>
@inproceedings{2020_CVPR_Kim,
    author = "Kim, Jinkyu and Moon, Suhong and Rohrbach, Anna and Darrell, Trevor and Canny, John",
    booktitle = "CVPR",
    title = "Advisable learning for self-driving vehicles by internalizing observation-to-action rules",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2018_ECCV_Kim></a>
<details close>
<summary>Kim et al., Textual Explanations for Self-Driving Vehicles, ECCV, 2018 | <a href=https://openaccess.thecvf.com/content_ECCV_2018/papers/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.pdf>paper</a> | <a href=https://github.com/JinkyuKimUCB/explainable-deep-driving>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-X>BDD-X</a>
</ul>
<ul>
<pre>
@inproceedings{2018_ECCV_Kim,
    author = "Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep",
    booktitle = "ECCV",
    title = "Textual explanations for self-driving vehicles",
    year = "2018"
}
</pre>
</ul>
</ul>
<ul><a name=2017_arXiv_Bojarski></a>
<details close>
<summary>Bojarski et al., Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car, arXiv, 2017 | <a href=https://arxiv.org/pdf/1704.07911.pdf>paper</a> | <a href=https://github.com/AutoDeep/PilotNet>code</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@article{2017_arXiv_Bojarski,
    author = "Bojarski, Mariusz and Yeres, Philip and Choromanska, Anna and Choromanski, Krzysztof and Firner, Bernhard and Jackel, Lawrence and Muller, Urs",
    journal = "arXiv:1704.07911",
    title = "Explaining how a deep neural network trained with end-to-end learning steers a car",
    year = "2017"
}
</pre>
</ul>
</ul>
