---
<a href=README.md/#top><l style="font-size:30px">Home</l></a>&nbsp;&nbsp;| <a href=behavioral.md><l style="font-size:30px">Behavioral</l></a>&nbsp;&nbsp;| <a href=scene_gaze.md><l style="font-size:30px">Applications</l></a>&nbsp;&nbsp;| <l style="font-size:35px">Datasets</l>&nbsp;&nbsp;
---

*Click on each entry below to see additional information.*
<a name="HOIST"></a>
<details close>
<summary>HOIST | <a href=https://doi.org/10.1109/LRA.2024.3368301>paper</a> | <a href=https://github.com/vehicle-importance/oiecr>link</a></summary>
<ul>
Full name: Object Importance Estimation Using Counterfactual Reasoning
</ul>
</summary>
<ul>
Description: Simulated driving scenarios with object importance annotations
</ul>
</summary>
<ul>
Data: scene video (BEV)
</ul>
</summary>
<ul>
Annotations: bounding boxes, object importance labels
</ul>
<ul>
<pre>
@article{2024_RAL_Gupta,
    author = "Gupta, Pranay and Biswas, Abhijat and Admoni, Henny and Held, David",
    journal = "IEEE Robotics and Automation Letters",
    publisher = "IEEE",
    title = "Object Importance Estimation using Counterfactual Reasoning for Intelligent Driving",
    year = "2024"
}
</pre>
</details>
</ul>

<a name="DRAMA"></a>
<details close>
<summary>DRAMA | <a href=https://openaccess.thecvf.com/content/WACV2023/papers/Malla_DRAMA_Joint_Risk_Localization_and_Captioning_in_Driving_WACV_2023_paper.pdf>paper</a> | <a href=https://usa.honda-ri.com/drama>link</a></summary>
<ul>
Full name: Driving Risk Assessment Mechanism with A captioning module
</ul>
</summary>
<ul>
Description: Driving scenarios recorded in Tokyo, Japan with video and object-level importance labels and captions
</ul>
</summary>
<ul>
Data: scene video
</ul>
</summary>
<ul>
Annotations: bounding boxes, captions
</ul>
<ul>
<pre>
@inproceedings{2023_WACV_Malla,
    author = "Malla, Srikanth and Choi, Chiho and Dwivedi, Isht and Choi, Joon Hee and Li, Jiachen",
    booktitle = "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision",
    pages = "1043--1052",
    title = "DRAMA: Joint Risk Localization and Captioning in Driving",
    year = "2023"
}
</pre>
</details>
</ul>

<a name="DrFixD-night"></a>
<details close>
<summary>DrFixD-night | <a href=https://doi.org/10.1109/TITS.2023.3323468>paper</a> | <a href=https://github.com/taodeng/DrFixD-night>link</a></summary>
<ul>
Full name: Driver Fixation Dataset in night
</ul>
</summary>
<ul>
Description: 15 videos of night-time driving with eye-tracking data from 30 participants
</ul>
</summary>
<ul>
Data: scene video, eye-tracking
</ul>
<ul>
<pre>
@article{2023_T-ITS_Deng,
    author = "Deng, Tao and Jiang, Lianfang and Shi, Yi and Wu, Jiang and Wu, Zhangbi and Yan, Shun and Zhang, Xianshi and Yan, Hongmei",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    publisher = "IEEE",
    title = "Driving Visual Saliency Prediction of Dynamic Night Scenes via a Spatio-Temporal Dual-Encoder Network",
    year = "2023"
}
</pre>
</details>
</ul>

<a name="AIDE"></a>
<details close>
<summary>AIDE | <a href=https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_AIDE_A_Vision-Driven_Multi-View_Multi-Modal_Multi-Tasking_Dataset_for_Assistive_Driving_ICCV_2023_paper.pdf>paper</a> | <a href=https://github.com/ydk122024/AIDE>link</a></summary>
<ul>
Full name: Assistive Driving Perception Dataset
</ul>
</summary>
<ul>
Description: Naturalistic dataset with multi-camera views of drivers performing normal driving and secondary tasks
</ul>
</summary>
<ul>
Data: driver video, scene video
</ul>
</summary>
<ul>
Annotations: distraction state, action labels
</ul>
<ul>
<pre>
@inproceedings{2023_ICCV_Yang,
    author = "Yang, Dingkang and Huang, Shuai and Xu, Zhi and Li, Zhenpeng and Wang, Shunli and Li, Mingcheng and Wang, Yuzheng and Liu, Yang and Yang, Kun and Chen, Zhaoyu and others",
    booktitle = "Proceedings of the IEEE/CVF International Conference on Computer Vision",
    pages = "20459--20470",
    title = "AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception",
    year = "2023"
}
</pre>
</details>
</ul>

<a name="SAM-DD"></a>
<details close>
<summary>SAM-DD | <a href=https://doi.org/10.1109/TITS.2023.3316203>paper</a> | <a href=https://yanghh.io/SAM-DD/>link</a></summary>
<ul>
Full name: Singapore AutoMan@NTU Distracted Driving Dataset
</ul>
</summary>
<ul>
Description: Videos of drivers performing secondary tasks
</ul>
</summary>
<ul>
Data: driver video, depth
</ul>
</summary>
<ul>
Annotations: distraction state
</ul>
<ul>
<pre>
@article{2023_T-ITS_Yang,
    author = "Yang, Haohan and Liu, Haochen and Hu, Zhongxu and Nguyen, Anh-Tu and Guerra, Thierry-Marie and Lv, Chen",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    publisher = "IEEE",
    title = "Quantitative Identification of Driver Distraction: A Weakly Supervised Contrastive Learning Approach",
    year = "2023"
}
</pre>
</details>
</ul>

<a name="100-Driver"></a>
<details close>
<summary>100-Driver | <a href=https://doi.org/10.1109/TITS.2023.3255923>paper</a> | <a href=https://100-driver.github.io>link</a></summary>
<ul>
Description: Videos of drivers performing secondary tasks 
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: action labels
</ul>
<ul>
<pre>
@article{2023_T-ITS_Wang,
    author = "Wang, Jing and Li, Wenjing and Li, Fang and Zhang, Jun and Wu, Zhongcheng and Zhong, Zhun and Sebe, Nicu",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    publisher = "IEEE",
    title = "100-Driver: A Large-Scale, Diverse Dataset for Distracted Driver Classification",
    year = "2023"
}
</pre>
</details>
</ul>

<a name="SynDD1"></a>
<details close>
<summary>SynDD1 | <a href=https://doi.org/10.1016/j.dib.2022.108793>paper</a> | <a href=https://data.mendeley.com/datasets/ptcp7rp3wb/4>link</a></summary>
<ul>
Full name: Synthetic Distracted Driving Dataset
</ul>
</summary>
<ul>
Description: Synthetic dataset for machine learning models to detect and analyze drivers' various distracted behavior and different gaze zones. 
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: gaze area labels, action labels, appearance labels
</ul>
<ul>
<pre>
@article{2023_DiB_Rahman,
    author = "Rahman, Mohammed Shaiqur and Venkatachalapathy, Archana and Sharma, Anuj and Wang, Jiyang and Gursoy, Senem Velipasalar and Anastasiu, David and Wang, Shuo",
    journal = "Data in brief",
    pages = "108793",
    publisher = "Elsevier",
    title = "Synthetic distracted driving (syndd1) dataset for analyzing distracted behaviors and various gaze zones of a driver",
    volume = "46",
    year = "2023"
}
</pre>
</details>
</ul>

<a name="Fatigueview"></a>
<details close>
<summary>Fatigueview | <a href=https://doi.org/10.1109/TITS.2022.3216017>paper</a> | <a href=https://fatigueview.github.io/>link</a></summary>
<ul>
Description: Multi-camera video dataset for vision-based drowsiness detection.
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: facial landmarks, face/hand bounding boxes, head pose, eye status, pose, drowsiness labels
</ul>
<ul>
<pre>
@article{2022_T-ITS_Yang,
    author = "Yang, Cong and Yang, Zhenyu and Li, Weiyu and See, John",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    publisher = "IEEE",
    title = "FatigueView: A Multi-Camera Video Dataset for Vision-Based Drowsiness Detection",
    year = "2022"
}
</pre>
</details>
</ul>

<a name="CoCAtt"></a>
<details close>
<summary>CoCAtt | <a href=https://doi.org/10.1109/ITSC55140.2022.9921777>paper</a> | <a href=https://cocatt-dataset.github.io/>link</a></summary>
<ul>
Full name: A Cognitive-Conditioned Driver Attention Dataset
</ul>
</summary>
<ul>
Description: Videos of drivers and driver scenes in automated and manual driving conditions with per-frame gaze and distraction annotations
</ul>
</summary>
<ul>
Data: driver video, scene video, eye-tracking
</ul>
</summary>
<ul>
Annotations: distraction state, car telemetry, intention labels
</ul>
<ul>
<pre>
@inproceedings{2022_ITSC_Shen,
    author = "Shen, Yuan and Wijayaratne, Niviru and Sriram, Pranav and Hasan, Aamir and Du, Peter and Driggs-Campbell, Katherine",
    booktitle = "2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)",
    organization = "IEEE",
    pages = "32--39",
    title = "CoCAtt: A Cognitive-Conditioned Driver Attention Dataset",
    year = "2022"
}
</pre>
</details>
</ul>

<a name="LBW"></a>
<details close>
<summary>LBW | <a href=https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136730128.pdf>paper</a> | <a href=https://github.com/Kasai2020/look_both_ways>link</a></summary>
<ul>
Full name: Look Both Ways
</ul>
</summary>
<ul>
Description: Synchronized videos from scene and driver-facing cameras of drivers performing various maneuvers in traffic
</ul>
</summary>
<ul>
Data: driver video, scene video, eye-tracking
</ul>
<ul>
<pre>
@inproceedings{2022_ECCV_Kasahara,
    author = "Kasahara, Isaac and Stent, Simon and Park, Hyun Soo",
    booktitle = "Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XIII",
    organization = "Springer",
    pages = "126--142",
    title = "Look Both Ways: Self-supervising Driver Gaze Estimation and Road Scene Saliency",
    year = "2022"
}
</pre>
</details>
</ul>

<a name="55 Rides"></a>
<details close>
<summary>55 Rides | <a href=https://doi.org/10.1145/3448018.3457993>paper</a> | <a href=https://www.hci.uni-tuebingen.de/research/Applications/Driving/55rides.html>link</a></summary>
<ul>
Description: Naturalistic dataset recorded by four drivers and annotated by three raters to determine distraction states
</ul>
</summary>
<ul>
Data: driver video, eye-tracking
</ul>
</summary>
<ul>
Annotations: distraction state, head pose
</ul>
<ul>
<pre>
@inproceedings{2021_ETRA_Kubler,
    author = {K{\"u}bler, Thomas C and Fuhl, Wolfgang and Wagner, Elena and Kasneci, Enkelejda},
    booktitle = "ACM Symposium on Eye Tracking Research and Applications",
    pages = "1--8",
    title = "55 Rides: attention annotated head and gaze data during naturalistic driving",
    year = "2021"
}
</pre>
</details>
</ul>

<a name="DAD"></a>
<details close>
<summary>DAD | <a href=https://openaccess.thecvf.com/content/WACV2021/papers/Kopuklu_Driver_Anomaly_Detection_A_Dataset_and_Contrastive_Learning_Approach_WACV_2021_paper.pdf>paper</a> | <a href=https://github.com/okankop/Driver-Anomaly-Detection>link</a></summary>
<ul>
Full name: Driver Anomaly Detection
</ul>
</summary>
<ul>
Description: Videos of normal and anomalous behaviors (manual/visual distractions) of drivers.
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: action labels
</ul>
<ul>
<pre>
@inproceedings{2021_WACV_Kopuklu,
    author = "Kopuklu, Okan and Zheng, Jiapeng and Xu, Hang and Rigoll, Gerhard",
    booktitle = "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision",
    pages = "91--100",
    title = "Driver anomaly detection: A dataset and contrastive learning approach",
    year = "2021"
}
</pre>
</details>
</ul>

<a name="MAAD"></a>
<details close>
<summary>MAAD | <a href=https://openaccess.thecvf.com/content/ICCV2021W/EPIC/papers/Gopinath_MAAD_A_Model_and_Dataset_for_Attended_Awareness_in_Driving_ICCVW_2021_paper.pdf>paper</a> | <a href=https://github.com/ToyotaResearchInstitute/att-aware/>link</a></summary>
<ul>
Full name: Attended Awareness in Driving
</ul>
</summary>
<ul>
Description: A subset of videos from DR(eye)VE annotated with gaze collected in lab conditions.
</ul>
</summary>
<ul>
Data: eye-tracking, scene video
</ul>
</summary>
<ul>
Annotations: task labels
</ul>
<ul>
<pre>
@inproceedings{2021_ICCVW_Gopinath,
    author = "Gopinath, Deepak and Rosman, Guy and Stent, Simon and Terahata, Katsuya and Fletcher, Luke and Argall, Brenna and Leonard, John",
    booktitle = "Proceedings of the IEEE/CVF International Conference on Computer Vision",
    pages = "3426--3436",
    title = {MAAD: A Model and Dataset for" Attended Awareness" in Driving},
    year = "2021"
}
</pre>
</details>
</ul>

<a name="DGW"></a>
<details close>
<summary>DGW | <a href=https://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Ghosh_Speak2Label_Using_Domain_Knowledge_for_Creating_a_Large_Scale_Driver_ICCVW_2021_paper.pdf>paper</a> | <a href=https://sites.google.com/view/drivergazeprediction/home>link</a></summary>
<ul>
Full name: Driver Gaze in the Wild 
</ul>
</summary>
<ul>
Description: Videos of drivers fixating on different areas in the vehicle without constraining their head and eye movements
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: gaze area labels
</ul>
<ul>
<pre>
@inproceedings{2021_ICCVW_Ghosh,
    author = "Ghosh, Shreya and Dhall, Abhinav and Sharma, Garima and Gupta, Sarthak and Sebe, Nicu",
    booktitle = "ICCVW",
    title = "Speak2label: Using domain knowledge for creating a large scale driver gaze zone estimation dataset",
    year = "2021"
}
</pre>
</details>
</ul>

<a name="TrafficSaliency"></a>
<details close>
<summary>TrafficSaliency | <a href=https://doi.org/10.1109/TITS.2019.2915540>paper</a> | <a href=https://github.com/taodeng/CDNN-traffic-saliency>link</a></summary>
<ul>
Description: 16 videos of driving scenes with gaze data of 28 subjects recorded in the lab with eye-tracker
</ul>
</summary>
<ul>
Data: eye-tracking, scene video
</ul>
<ul>
<pre>
@article{2020_T-ITS_Deng,
    author = "Deng, Tao and Yan, Hongmei and Qin, Long and Ngo, Thuyen and Manjunath, BS",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "5",
    pages = "2146--2154",
    publisher = "IEEE",
    title = "{How do drivers allocate their potential attention? Driving fixation prediction via convolutional neural networks}",
    volume = "21",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="NeuroIV"></a>
<details close>
<summary>NeuroIV | <a href=https://doi.org/10.1109/TITS.2020.3022921>paper</a> | <a href=https://github.com/ispc-lab/NeuroIV>link</a></summary>
<ul>
Full name: Neuromorphic Vision Meets Intelligent Vehicle
</ul>
</summary>
<ul>
Description: Videos of drivers performing secondary tasks, making hand gestures and observing different regions inside the vehicle recorded with DAVIS and depth sensor
</ul>
</summary>
<ul>
Data: driver video
</ul>
<ul>
<pre>
@article{2020_T-ITS_Chen,
    author = {Chen, Guang and Wang, Fa and Li, Weijun and Hong, Lin and Conradt, J{\"o}rg and Chen, Jieneng and Zhang, Zhenyan and Lu, Yiwen and Knoll, Alois},
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "2",
    pages = "1171--1183",
    publisher = "IEEE",
    title = "NeuroIV: Neuromorphic vision meets intelligent vehicle towards safe driving with a new database and baseline evaluations",
    volume = "23",
    year = "2020"
}
</pre>
</details>
</ul>

<a name="LISA v2"></a>
<details close>
<summary>LISA v2 | <a href=https://doi.org/10.1109/IV47402.2020.9304573>paper</a> | <a href=https://github.com/arangesh/GPCycleGAN>link</a></summary>
<ul>
Full name: Laboratory for Intelligent and Safe Automobiles
</ul>
</summary>
<ul>
Description: Videos of drivers with and without eyeglasses recorded under different lighting conditions
</ul>
</summary>
<ul>
Data: driver video
</ul>
<ul>
<pre>
@inproceedings{2020_IV_Rangesh,
    author = "Rangesh, Akshay and Zhang, Bowen and Trivedi, Mohan M",
    booktitle = "IV",
    title = "Driver gaze estimation in the real world: Overcoming the eyeglass challenge",
    year = "2020"
}
</pre>
</details>
</ul>

<a name="DGAZE"></a>
<details close>
<summary>DGAZE | <a href=http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/ConferencePapers/2020/DGAZE_Driver.pdf>paper</a> | <a href=https://github.com/duaisha/DGAZE>link</a></summary>
<ul>
Description: A dataset mapping drivers’ gaze to different areas in a static traffic scene in lab conditions
</ul>
</summary>
<ul>
Data: driver video, scene video
</ul>
</summary>
<ul>
Annotations: bounding boxes
</ul>
<ul>
<pre>
@inproceedings{2020_IROS_Dua,
    author = "Dua, Isha and John, Thrupthi Ann and Gupta, Riya and Jawahar, CV",
    booktitle = "IROS",
    title = "DGAZE: Driver Gaze Mapping on Road",
    year = "2020"
}
</pre>
</details>
</ul>

<a name="DMD"></a>
<details close>
<summary>DMD | <a href=https://doi.org/10.1007/978-3-030-66823-5_23>paper</a> | <a href=https://dmd.vicomtech.org/>link</a></summary>
<ul>
Full name: Driving Monitoring Dataset
</ul>
</summary>
<ul>
Description: A diverse multi-modal dataset of drivers performing various secondary tasks, observing different regions inside the car, and showing signs of drowsiness recorded on-road and in simulation environment
</ul>
</summary>
<ul>
Data: driver video, scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: bounding boxes, action labels
</ul>
<ul>
<pre>
@inproceedings{2020_ECCVW_Ortega,
    author = "Ortega, Juan Diego and Kose, Neslihan and Ca{\\textasciitilde n}as, Paola and Chao, Min-An and Unnervik, Alexander and Nieto, Marcos and Otaegui, Oihana and Salgado, Luis",
    booktitle = "ECCV",
    title = "Dmd: A large-scale multi-modal driver monitoring dataset for attention and alertness analysis",
    year = "2020"
}
</pre>
</details>
</ul>

<a name="PRORETA 4"></a>
<details close>
<summary>PRORETA 4 | <a href=https://doi.org/10.1109/IVS.2019.8814224>paper</a> | <a href=https://www.proreta.tu-darmstadt.de/proreta_1_4/proreta4_1/datasets_1/index.en.jsp>link</a></summary>
<ul>
Description: Videos of traffic scenes recorded in instrumented vehicle with driver’s gaze data for evaluating accuracy of detecting driver’s current object of fixation
</ul>
</summary>
<ul>
Data: eye-tracking, driver video, scene video
</ul>
<ul>
<pre>
@inproceedings{2019_IV_Schwehr,
    author = "Schwehr, Julian and Knaust, Moritz and Willert, Volker",
    booktitle = "IV",
    title = "How to evaluate object-of-fixation detection",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="DADA-2000"></a>
<details close>
<summary>DADA-2000 | <a href=https://doi.org/10.1109/ITSC.2019.8917218>paper</a> | <a href=https://github.com/JWFangit/LOTVS-DADA>link</a></summary>
<ul>
Full name: Driver Attention in Driving Accident Scenarios
</ul>
</summary>
<ul>
Description: 2000 videos of accident videos collected from video hosting websites with eye-tracking data from 20 subjects collected in the lab.
</ul>
</summary>
<ul>
Data: eye-tracking, scene video
</ul>
</summary>
<ul>
Annotations: bounding boxes, accident category labels
</ul>
<ul>
<pre>
@inproceedings{2019_ITSC_Fang,
    author = "Fang, Jianwu and Yan, Dingxin and Qiao, Jiahuan and Xue, Jianru and Wang, He and Li, Sen",
    booktitle = "ITSC",
    title = "{DADA-2000: Can Driving Accident be Predicted by Driver Attentionƒ Analyzed by A Benchmark}",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="Drive&Act"></a>
<details close>
<summary>Drive&Act | <a href=https://openaccess.thecvf.com/content_ICCV_2019/papers/Martin_DriveAct_A_Multi-Modal_Dataset_for_Fine-Grained_Driver_Behavior_Recognition_in_ICCV_2019_paper.pdf>paper</a> | <a href=https://www.driveandact.com/>link</a></summary>
<ul>
Description: Videos of drivers performing various driving- and non-driving-related tasks
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: semantic maps, action labels
</ul>
<ul>
<pre>
@inproceedings{2019_ICCV_Martin,
    author = "Martin, Manuel and Roitberg, Alina and Haurilet, Monica and Horne, Matthias and Rei{\ss}, Simon and Voit, Michael and Stiefelhagen, Rainer",
    booktitle = "ICCV",
    title = "Drive\\&act: A multi-modal dataset for fine-grained driver behavior recognition in autonomous vehicles",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="RLDD"></a>
<details close>
<summary>RLDD | <a href=https://openaccess.thecvf.com/content_CVPRW_2019/papers/AMFG/Ghoddoosian_A_Realistic_Dataset_and_Baseline_Temporal_Model_for_Early_Drowsiness_CVPRW_2019_paper.pdf>paper</a> | <a href=https://github.com/rezaghoddoosian/Early-Drowsiness-Detection>link</a></summary>
<ul>
Full name: Real-Life Drowsiness Datase
</ul>
</summary>
<ul>
Description: Crowdsourced videos of people in various states of drowsiness recorded in indoor environments
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: drowsiness labels
</ul>
<ul>
<pre>
@inproceedings{2019_CVPRW_Ghoddoosian,
    author = "Ghoddoosian, Reza and Galib, Marnim and Athitsos, Vassilis",
    booktitle = "CVPRW",
    title = "A realistic dataset and baseline temporal model for early drowsiness detection",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="HAD"></a>
<details close>
<summary>HAD | <a href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Grounding_Human-To-Vehicle_Advice_for_Self-Driving_Vehicles_CVPR_2019_paper.pdf>paper</a> | <a href=https://usa.honda-ri.com/HAD>link</a></summary>
<ul>
Full name: HAD HRI Advice Dataset
</ul>
</summary>
<ul>
Description: A subset of videos from HDD naturalistic dataset annotated with textual advice containing 1) goals – where the vehicle should move and 2) attention – where the vehicle should look
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: goal and attention labels
</ul>
<ul>
<pre>
@inproceedings{2019_CVPR_Kim,
    author = "Kim, Jinkyu and Misu, Teruhisa and Chen, Yi-Ting and Tawari, Ashish and Canny, John",
    booktitle = "CVPR",
    title = "Grounding human-to-vehicle advice for self-driving vehicles",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="3MDAD"></a>
<details close>
<summary>3MDAD | <a href=https://doi.org/10.1007/978-3-030-29888-3_42>paper</a> | <a href=https://sites.google.com/site/benkhalifaanouar1/6-datasets>link</a></summary>
<ul>
Full name: Multimodal Multiview and Multispectral Driver Action Dataset
</ul>
</summary>
<ul>
Description: Videos of drivers performing secondary tasks
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: action labels, bounding boxes
</ul>
<ul>
<pre>
@inproceedings{2019_CAIP_Jegham,
    author = "Jegham, Imen and Ben Khalifa, Anouar and Alouani, Ihsen and Mahjoub, Mohamed Ali",
    booktitle = "Computer Analysis of Images and Patterns: 18th International Conference, CAIP 2019, Salerno, Italy, September 3--5, 2019, Proceedings, Part I 18",
    organization = "Springer",
    pages = "518--529",
    title = "Mdad: A multimodal and multiview in-vehicle driver action dataset",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="EBDD"></a>
<details close>
<summary>EBDD | <a href=https://doi.org/10.1109/TCSVT.2018.2818407>paper</a> | <a href=https://mahbubur.buet.ac.bd/resources/ebdd_database.htm>link</a></summary>
<ul>
Full name: EEE BUET Distracted Driving Dataset
</ul>
</summary>
<ul>
Description: Videos of drivers performing secondary tasks
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: action labels, bounding boxes
</ul>
<ul>
<pre>
@article{2019_TCSVT_Billah,
    author = "Billah, Tashrif and Rahman, SM Mahbubur and Ahmad, M Omair and Swamy, MNS",
    journal = "IEEE Transactions on Circuits and Systems for Video Technology",
    number = "4",
    pages = "1048--1062",
    publisher = "IEEE",
    title = "Recognizing distractions for assistive driving by tracking body parts",
    volume = "29",
    year = "2018"
}
</pre>
</details>
</ul>

<a name="H3D"></a>
<details close>
<summary>H3D | <a href=https://doi.org/10.1109/ICRA.2019.8793925>paper</a> | <a href=https://doi.org/10.1109/ICRA.2019.8793925>link</a></summary>
<ul>
Full name: H3D Honda 3D Dataset
</ul>
</summary>
<ul>
Description: A subset of videos from HDD dataset with 3D bounding boxes and object ids for tracking
</ul>
</summary>
<ul>
Data: driver video, vehicle data
</ul>
</summary>
<ul>
Annotations: bounding boxes
</ul>
<ul>
<pre>
@inproceedings{2019_ICRA_Patil,
    author = "Patil, Abhishek and Malla, Srikanth and Gang, Haiming and Chen, Yi-Ting",
    booktitle = "2019 International Conference on Robotics and Automation (ICRA)",
    organization = "IEEE",
    pages = "9552--9557",
    title = "The h3d dataset for full-surround 3d multi-object detection and tracking in crowded urban scenes",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="DR(eye)VE"></a>
<details close>
<summary>DR(eye)VE | <a href=https://doi.org/10.1109/TPAMI.2018.2845370>paper</a> | <a href=http://imagelab.ing.unimore.it/dreyeve>link</a></summary>
<ul>
Description: Driving videos recorded on-road with corresponding gaze data of the driver
</ul>
</summary>
<ul>
Data: eye-tracking, scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: weather and road type labels
</ul>
<ul>
<pre>
@article{2018_PAMI_Palazzi,
    author = "Palazzi, Andrea and Abati, Davide and Solera, Francesco and Cucchiara, Rita and others",
    journal = "IEEE TPAMI",
    number = "7",
    pages = "1720--1733",
    title = "{Predicting the Driver's Focus of Attention: the DR (eye) VE Project}",
    volume = "41",
    year = "2018"
}
</pre>
</details>
</ul>

<a name="BDD-X"></a>
<details close>
<summary>BDD-X | <a href=https://openaccess.thecvf.com/content_ECCV_2018/papers/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.pdf>paper</a> | <a href=https://github.com/JinkyuKimUCB/BDD-X-dataset>link</a></summary>
<ul>
Full name: Berkeley Deep Drive-X (eXplanation) Dataset
</ul>
</summary>
<ul>
Description: A subset of videos from BDD dataset annotated with textual descriptions of actions performed by the vehicle and explanations justifying those actions
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: action explanations
</ul>
<ul>
<pre>
@inproceedings{2018_ECCV_Kim,
    author = "Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep",
    booktitle = "ECCV",
    title = "Textual explanations for self-driving vehicles",
    year = "2018"
}
</pre>
</details>
</ul>

<a name="HDD"></a>
<details close>
<summary>HDD | <a href=https://openaccess.thecvf.com/content_cvpr_2018/papers/Ramanishka_Toward_Driving_Scene_CVPR_2018_paper.pdf>paper</a> | <a href=https://usa.honda-ri.com/HDD>link</a></summary>
<ul>
Full name: HDD HRI Driving Dataset
</ul>
</summary>
<ul>
Description: A large naturalistic driving dataset with driving footage, vehicle telemetry and annotations for vehicle actions and their justifications
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: bounding boxes, action labels
</ul>
<ul>
<pre>
@inproceedings{2018_CVPR_Ramanishka,
    author = "Ramanishka, Vasili and Chen, Yi-Ting and Misu, Teruhisa and Saenko, Kate",
    booktitle = "CVPR",
    title = "Toward driving scene understanding: A dataset for learning driver behavior and causal reasoning",
    year = "2018"
}
</pre>
</details>
</ul>

<a name="BDD-A"></a>
<details close>
<summary>BDD-A | <a href=https://doi.org/10.1007/978-3-030-20873-8_42>paper</a> | <a href=https://bdd-data.berkeley.edu/>link</a></summary>
<ul>
Full name: Berkeley Deep Drive-A (Attention) Dataset
</ul>
</summary>
<ul>
Description: A set of short video clips extracted from the Berkeley Deep Drive (BDD) dataset with additional eye-tracking data collected in the lab from 45 subjects
</ul>
</summary>
<ul>
Data: eye-tracking, scene video, vehicle data
</ul>
<ul>
<pre>
@inproceedings{2018_ACCV_Xia,
    author = "Xia, Ye and Zhang, Danqing and Kim, Jinkyu and Nakayama, Ken and Zipser, Karl and Whitney, David",
    booktitle = "ACCV",
    title = "Predicting driver attention in critical situations",
    year = "2018"
}
</pre>
</details>
</ul>

<a name="C42CN"></a>
<details close>
<summary>C42CN | <a href=https://doi.org/10.1038/sdata.2017.110>paper</a> | <a href=https://osf.io/c42cn/>link</a></summary>
<ul>
Description: A multi-modal dataset acquired in a controlled experiment on a driving simulator under 4 conditions: no distraction, cognitive, emotional and sensorimotor distraction.
</ul>
</summary>
<ul>
Data: eye-tracking, scene video, physiological signal
</ul>
<ul>
<pre>
@article{2017_NatSciData_Taamneh,
    author = "Taamneh, Salah and Tsiamyrtzis, Panagiotis and Dcosta, Malcolm and Buddharaju, Pradeep and Khatri, Ashik and Manser, Michael and Ferris, Thomas and Wunderlich, Robert and Pavlidis, Ioannis",
    journal = "Scientific Data",
    pages = "170110",
    title = "A multimodal dataset for various forms of distracted driving",
    volume = "4",
    year = "2017"
}
</pre>
</details>
</ul>

<a name="DriveAHead"></a>
<details close>
<summary>DriveAHead | <a href=https://openaccess.thecvf.com/content_cvpr_2017_workshops/w13/papers/Schwarz_DriveAHead_-_A_CVPR_2017_paper.pdf>paper</a> | <a href=https://cvhci.anthropomatik.kit.edu/data/DriveAHead/>link</a></summary>
<ul>
Description: Videos of drivers with frame-level head pose annotations obtained from a motion-capture system
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: occlusion, head pose, depth
</ul>
<ul>
<pre>
@inproceedings{2017_CVPRW_Schwarz,
    author = "Schwarz, Anke and Haurilet, Monica and Martinez, Manuel and Stiefelhagen, Rainer",
    booktitle = "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops",
    pages = "1--10",
    title = "Driveahead-a large-scale driver head pose dataset",
    year = "2017"
}
</pre>
</details>
</ul>

<a name="DDD"></a>
<details close>
<summary>DDD | <a href=https://doi.org/10.1007/978-3-319-54526-4_9>paper</a> | <a href=http://cv.cs.nthu.edu.tw/php/callforpaper/datasets/DDD/>link</a></summary>
<ul>
Full name: Driver Drowsiness Detection Dataset
</ul>
</summary>
<ul>
Description: Videos of human subjects simulating different levels of drowsiness while driving in a simulator
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: drowsiness labels
</ul>
<ul>
<pre>
@inproceedings{2017_ACCV_Weng,
    author = "Weng, Ching-Hua and Lai, Ying-Hsiu and Lai, Shang-Hong",
    booktitle = "ACCV",
    title = "Driver drowsiness detection via a hierarchical temporal deep belief network",
    year = "2016"
}
</pre>
</details>
</ul>

<a name="Dashcam dataset"></a>
<details close>
<summary>Dashcam dataset | <a href=https://github.com/SullyChen/driving-datasets>link</a></summary>
<ul>
Description: Driving videos with steering information recorded on road
</ul>
</summary>
<ul>
Data: scene video
</ul>
<ul>
<pre>
</pre>
</details>
</ul>

<a name="AUCD2"></a>
<details close>
<summary>AUCD2 | <a href=https://arxiv.org/pdf/1706.09498.pdf>paper</a> | <a href=https://abouelnaga.io/projects/auc-distracted-driver-dataset/>link</a></summary>
<ul>
Full name: American University in Cairo (AUC) Distracted Driver’s Dataset
</ul>
</summary>
<ul>
Description: Videos of drivers performing secondary tasks
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: action labels
</ul>
<ul>
<pre>
@inproceedings{2017_NeurIPS_Abouelnaga,
    author = "Abouelnaga, Yehya and Eraqi, Hesham M. and Moustafa, Mohamed N.",
    booktitle = "NeurIPS Workshop on Machine Learning for Intelligent Transportation Systems",
    title = "eal-time Distracted Driver Posture Classification",
    year = "2017"
}
</pre>
</details>
</ul>

<a name="DROZY"></a>
<details close>
<summary>DROZY | <a href=https://doi.org/10.1109/WACV.2016.7477715>paper</a> | <a href=http://www.drozy.ulg.ac.be/>link</a></summary>
<ul>
Description: Videos and physiological data from subjects in different drowsiness states after prolonged waking
</ul>
</summary>
<ul>
Data: driver video, physiological signal
</ul>
</summary>
<ul>
Annotations: drowsiness labels
</ul>
<ul>
<pre>
@inproceedings{2016_WACV_Massoz,
    author = "Massoz, Quentin and Langohr, Thomas and Fran{\c{c}}ois, Cl{\'e}mentine and Verly, Jacques G",
    booktitle = "WACV",
    title = "The ULg multimodality drowsiness database (called DROZY) and examples of use",
    year = "2016"
}
</pre>
</details>
</ul>

<a name="TETD"></a>
<details close>
<summary>TETD | <a href=https://doi.org/10.1109/TITS.2016.2535402>paper</a> | <a href=https://github.com/taodeng/traffic-eye-tracking-dataset>link</a></summary>
<ul>
Full name: Traffic Eye Tracking Dataset
</ul>
</summary>
<ul>
Description: A set of 100 images of traffic scenes with corresponding eye-tracking data from 20 subjects
</ul>
</summary>
<ul>
Data: eye-tracking, scene images
</ul>
<ul>
<pre>
@article{2016_T-ITS_Deng,
    author = "Deng, Tao and Yang, Kaifu and Li, Yongjie and Yan, Hongmei",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "7",
    pages = "2051--2062",
    publisher = "IEEE",
    title = "Where does the driver look? Top-down-based saliency detection in a traffic driving environment",
    volume = "17",
    year = "2016"
}
</pre>
</details>
</ul>

<a name="DAD"></a>
<details close>
<summary>DAD | <a href=https://doi.org/10.1007/978-3-319-54190-7_9>paper</a> | <a href=https://aliensunmin.github.io/project/dashcam/>link</a></summary>
<ul>
Description: Videos of accidents recorded with dashboard cameras sourced from video hosting sites with annotations for accidents and road users involved in them
</ul>
</summary>
<ul>
Data: scene video
</ul>
</summary>
<ul>
Annotations: bounding boxes, accident category labels
</ul>
<ul>
<pre>
@inproceedings{2016_ACCV_Chan,
    author = "Chan, Fu-Hsiang and Chen, Yu-Ting and Xiang, Yu and Sun, Min",
    booktitle = "ACCV",
    title = "Anticipating accidents in dashcam videos",
    year = "2016"
}
</pre>
</details>
</ul>

<a name="Brain4Cars"></a>
<details close>
<summary>Brain4Cars | <a href=https://openaccess.thecvf.com/content_iccv_2015/papers/Jain_Car_That_Knows_ICCV_2015_paper.pdf>paper</a> | <a href=https://github.com/asheshjain399/ICCV2015_Brain4Cars>link</a></summary>
<ul>
Description: Synchronized videos from scene and driver-facing cameras of drivers performing various maneuvers in traffic
</ul>
</summary>
<ul>
Data: driver video, scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: action labels
</ul>
<ul>
<pre>
@inproceedings{2015_ICCV_Jain,
    author = "Jain, Ashesh and Koppula, Hema S and Raghavan, Bharad and Soh, Shane and Saxena, Ashutosh",
    booktitle = "ICCV",
    title = "Car that knows before you do: Anticipating maneuvers via learning temporal driving models",
    year = "2015"
}
</pre>
</details>
</ul>

<a name="SFD"></a>
<details close>
<summary>SFD | <a href=https://www.kaggle.com/competitions/state-farm-distracted-driver-detection/overview>link</a></summary>
<ul>
Full name: State Farm Distracted Driver Detection
</ul>
</summary>
<ul>
Description: Videos of drivers performing secondary tasks
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: action labels
</ul>
<ul>
<pre>
</pre>
</details>
</ul>

<a name="DIPLECS Surrey"></a>
<details close>
<summary>DIPLECS Surrey | <a href=https://doi.org/10.1109/TVT.2015.2487826>paper</a> | <a href=https://cvssp.org/data/diplecs/>link</a></summary>
<ul>
Description: Driving videos with steering information recorded in different cars and environments
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
<ul>
<pre>
@article{2015_TranVehTech_Pugeault,
    author = "Pugeault, Nicolas and Bowden, Richard",
    journal = "IEEE Transactions on Vehicular Technology",
    number = "12",
    pages = "5424--5438",
    publisher = "IEEE",
    title = "How much of driving is preattentive?",
    volume = "64",
    year = "2015"
}
</pre>
</details>
</ul>

<a name="YawDD"></a>
<details close>
<summary>YawDD | <a href=https://doi.org/10.1145/2557642.2563678>paper</a> | <a href=https://ieee-dataport.org/open-access/yawdd-yawning-detection-dataset>link</a></summary>
<ul>
Full name: Yawning Detection Dataset
</ul>
</summary>
<ul>
Description: Recordings of human subjects in parked vehicles simulating normal driving, singing and taslking, and yawning
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: bounding boxes, action labels
</ul>
<ul>
<pre>
@inproceedings{2014_ACM_Abtahi,
    author = "Abtahi, Shabnam and Omidyeganeh, Mona and Shirmohammadi, Shervin and Hariri, Behnoosh",
    booktitle = "Proceedings of the ACM Multimedia Systems Conference",
    title = "{YawDD: A yawning detection dataset}",
    year = "2014"
}
</pre>
</details>
</ul>

<a name="3DDS"></a>
<details close>
<summary>3DDS | <a href=http://www.bmva.org/bmvc/2011/proceedings/paper85/paper85.pdf>paper</a> | <a href=http://ilab.usc.edu/borji/Resources.html>link</a></summary>
<ul>
Full name: 3D Driving School Dataset
</ul>
</summary>
<ul>
Description: Videos and eye-tracking data of people playing 3D driving simulator game
</ul>
</summary>
<ul>
Data: eye-tracking, scene video
</ul>
<ul>
<pre>
@inproceedings{2011_BMVC_Borji,
    author = "Borji, Ali and Sihite, Dicky N and Itti, Laurent",
    booktitle = "BMVC",
    title = "Computational Modeling of Top-down Visual Attention in Interactive Environments.",
    year = "2011"
}
</pre>
</details>
</ul>

<a name="DIPLECS Sweden"></a>
<details close>
<summary>DIPLECS Sweden | <a href=https://doi.org/10.1007/978-3-642-15567-3_12>paper</a> | <a href=https://cvssp.org/data/diplecs/>link</a></summary>
<ul>
Description: Driving videos with steering information recorded in different cars and environments
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
<ul>
<pre>
@inproceedings{2010_ACCV_Pugeault,
    author = "Pugeault, Nicolas and Bowden, Richard",
    booktitle = "ECCV",
    title = "Learning pre-attentive driving behaviour from holistic visual features",
    year = "2010"
}
</pre>
</details>
</ul>

<a name="BU HeadTracking"></a>
<details close>
<summary>BU HeadTracking | <a href=https://doi.org/10.1109/34.845375>paper</a> | <a href=https://www.cs.bu.edu/groups/ivc/HeadTracking/>link</a></summary>
<ul>
Full name: Boston University Head Tracking Dataset
</ul>
</summary>
<ul>
Description: Videos and head tracking information for multiple human subjects recorded in diverse conditions
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: head pose
</ul>
<ul>
<pre>
@article{2000_PAMI_LaCascia,
    author = "La Cascia, Marco and Sclaroff, Stan and Athitsos, Vassilis",
    journal = "IEEE Transactions on pattern analysis and machine intelligence",
    number = "4",
    pages = "322--336",
    publisher = "IEEE",
    title = "Fast, reliable head tracking under varying illumination: An approach based on registration of texture-mapped 3D models",
    volume = "22",
    year = "2000"
}
</pre>
</details>
</ul>

