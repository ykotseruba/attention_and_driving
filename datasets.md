---
<a href=README.md/#top><l style="font-size:30px">Home</l></a>&nbsp;&nbsp;| <a href=behavioral.md><l style="font-size:30px">Behavioral</l></a>&nbsp;&nbsp;| <a href=scene_gaze.md><l style="font-size:30px">Applications</l></a>&nbsp;&nbsp;| <l style="font-size:35px">Datasets</l>&nbsp;&nbsp;
---

*Click on each entry below to see additional information.*
<a name="100-Driver"></a>
<details close>
<summary>100-Driver | <a href=https://doi.org/10.1109/TITS.2023.3255923>paper</a> | <a href=https://100-driver.github.io>link</a></summary>
<ul>
Description: Videos of drivers performing secondary tasks 
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: action labels
</ul>
<ul>
<pre>
@article{2023_T-ITS_Wang,
    author = "Wang, Jing and Li, Wenjing and Li, Fang and Zhang, Jun and Wu, Zhongcheng and Zhong, Zhun and Sebe, Nicu",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    publisher = "IEEE",
    title = "100-Driver: A Large-Scale, Diverse Dataset for Distracted Driver Classification",
    year = "2023"
}
</pre>
</details>
</ul>

<a name="SynDD1"></a>
<details close>
<summary>SynDD1 | <a href=https://doi.org/10.1016/j.dib.2022.108793>paper</a> | <a href=https://data.mendeley.com/datasets/ptcp7rp3wb/4>link</a></summary>
<ul>
Full name: Synthetic Distracted Driving Dataset
</ul>
</summary>
<ul>
Description: Synthetic dataset for machine learning models to detect and analyze drivers' various distracted behavior and different gaze zones. 
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: gaze area labels, action labels, appearance labels
</ul>
<ul>
<pre>
@article{2023_DiB_Rahman,
    author = "Rahman, Mohammed Shaiqur and Venkatachalapathy, Archana and Sharma, Anuj and Wang, Jiyang and Gursoy, Senem Velipasalar and Anastasiu, David and Wang, Shuo",
    journal = "Data in brief",
    pages = "108793",
    publisher = "Elsevier",
    title = "Synthetic distracted driving (syndd1) dataset for analyzing distracted behaviors and various gaze zones of a driver",
    volume = "46",
    year = "2023"
}
</pre>
</details>
</ul>

<a name="Fatigueview"></a>
<details close>
<summary>Fatigueview | <a href=https://doi.org/10.1109/TITS.2022.3216017>paper</a> | <a href=https://fatigueview.github.io/>link</a></summary>
<ul>
Description: Multi-camera video dataset for vision-based drowsiness detection.
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: facial landmarks, face/hand bounding boxes, head pose, eye status, pose, drowsiness labels
</ul>
<ul>
<pre>
@article{2022_T-ITS_Yang,
    author = "Yang, Cong and Yang, Zhenyu and Li, Weiyu and See, John",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    publisher = "IEEE",
    title = "FatigueView: A Multi-Camera Video Dataset for Vision-Based Drowsiness Detection",
    year = "2022"
}
</pre>
</details>
</ul>

<a name="CoCAtt"></a>
<details close>
<summary>CoCAtt | <a href=https://doi.org/10.1109/ITSC55140.2022.9921777>paper</a> | <a href=https://cocatt-dataset.github.io/>link</a></summary>
<ul>
Full name: A Cognitive-Conditioned Driver Attention Dataset
</ul>
</summary>
<ul>
Description: Videos of drivers and driver scenes in automated and manual driving conditions with per-frame gaze and distraction annotations
</ul>
</summary>
<ul>
Data: driver video, scene video, eye-tracking
</ul>
</summary>
<ul>
Annotations: distraction state, car telemetry, intention labels
</ul>
<ul>
<pre>
@inproceedings{2022_ITSC_Shen,
    author = "Shen, Yuan and Wijayaratne, Niviru and Sriram, Pranav and Hasan, Aamir and Du, Peter and Driggs-Campbell, Katherine",
    booktitle = "2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)",
    organization = "IEEE",
    pages = "32--39",
    title = "CoCAtt: A Cognitive-Conditioned Driver Attention Dataset",
    year = "2022"
}
</pre>
</details>
</ul>

<a name="LBW"></a>
<details close>
<summary>LBW | <a href=https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136730128.pdf>paper</a> | <a href=https://github.com/Kasai2020/look_both_ways>link</a></summary>
<ul>
Full name: Look Both Ways
</ul>
</summary>
<ul>
Description: Synchronized videos from scene and driver-facing cameras of drivers performing various maneuvers in traffic
</ul>
</summary>
<ul>
Data: driver video, scene video, eye-tracking
</ul>
<ul>
<pre>
@inproceedings{2022_ECCV_Kasahara,
    author = "Kasahara, Isaac and Stent, Simon and Park, Hyun Soo",
    booktitle = "Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XIII",
    organization = "Springer",
    pages = "126--142",
    title = "Look Both Ways: Self-supervising Driver Gaze Estimation and Road Scene Saliency",
    year = "2022"
}
</pre>
</details>
</ul>

<a name="55 Rides"></a>
<details close>
<summary>55 Rides | <a href=https://doi.org/10.1145/3448018.3457993>paper</a> | <a href=https://www.hci.uni-tuebingen.de/research/Applications/Driving/55rides.html>link</a></summary>
<ul>
Description: Naturalistic dataset recorded by four drivers and annotated by three raters to determine distraction states
</ul>
</summary>
<ul>
Data: driver video, eye-tracking
</ul>
</summary>
<ul>
Annotations: distraction state, head pose
</ul>
<ul>
<pre>
@inproceedings{2021_ETRA_Kubler,
    author = {K{\"u}bler, Thomas C and Fuhl, Wolfgang and Wagner, Elena and Kasneci, Enkelejda},
    booktitle = "ACM Symposium on Eye Tracking Research and Applications",
    pages = "1--8",
    title = "55 Rides: attention annotated head and gaze data during naturalistic driving",
    year = "2021"
}
</pre>
</details>
</ul>

<a name="DAD"></a>
<details close>
<summary>DAD | <a href=https://openaccess.thecvf.com/content/WACV2021/papers/Kopuklu_Driver_Anomaly_Detection_A_Dataset_and_Contrastive_Learning_Approach_WACV_2021_paper.pdf>paper</a> | <a href=https://github.com/okankop/Driver-Anomaly-Detection>link</a></summary>
<ul>
Full name: Driver Anomaly Detection
</ul>
</summary>
<ul>
Description: Videos of normal and anomalous behaviors (manual/visual distractions) of drivers.
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: action labels
</ul>
<ul>
<pre>
@inproceedings{2021_WACV_Kopuklu,
    author = "Kopuklu, Okan and Zheng, Jiapeng and Xu, Hang and Rigoll, Gerhard",
    booktitle = "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision",
    pages = "91--100",
    title = "Driver anomaly detection: A dataset and contrastive learning approach",
    year = "2021"
}
</pre>
</details>
</ul>

<a name="MAAD"></a>
<details close>
<summary>MAAD | <a href=https://openaccess.thecvf.com/content/ICCV2021W/EPIC/papers/Gopinath_MAAD_A_Model_and_Dataset_for_Attended_Awareness_in_Driving_ICCVW_2021_paper.pdf>paper</a> | <a href=https://github.com/ToyotaResearchInstitute/att-aware/>link</a></summary>
<ul>
Full name: Attended Awareness in Driving
</ul>
</summary>
<ul>
Description: A subset of videos from DR(eye)VE annotated with gaze collected in lab conditions.
</ul>
</summary>
<ul>
Data: eye-tracking, scene video
</ul>
</summary>
<ul>
Annotations: task labels
</ul>
<ul>
<pre>
@inproceedings{2021_ICCVW_Gopinath,
    author = "Gopinath, Deepak and Rosman, Guy and Stent, Simon and Terahata, Katsuya and Fletcher, Luke and Argall, Brenna and Leonard, John",
    booktitle = "Proceedings of the IEEE/CVF International Conference on Computer Vision",
    pages = "3426--3436",
    title = {MAAD: A Model and Dataset for" Attended Awareness" in Driving},
    year = "2021"
}
</pre>
</details>
</ul>

<a name="DGW"></a>
<details close>
<summary>DGW | <a href=https://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Ghosh_Speak2Label_Using_Domain_Knowledge_for_Creating_a_Large_Scale_Driver_ICCVW_2021_paper.pdf>paper</a> | <a href=https://sites.google.com/view/drivergazeprediction/home>link</a></summary>
<ul>
Full name: Driver Gaze in the Wild 
</ul>
</summary>
<ul>
Description: Videos of drivers fixating on different areas in the vehicle without constraining their head and eye movements
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: gaze area labels
</ul>
<ul>
<pre>
@inproceedings{2021_ICCVW_Ghosh,
    author = "Ghosh, Shreya and Dhall, Abhinav and Sharma, Garima and Gupta, Sarthak and Sebe, Nicu",
    booktitle = "ICCVW",
    title = "Speak2label: Using domain knowledge for creating a large scale driver gaze zone estimation dataset",
    year = "2021"
}
</pre>
</details>
</ul>

<a name="TrafficSaliency"></a>
<details close>
<summary>TrafficSaliency | <a href=https://doi.org/10.1109/TITS.2019.2915540>paper</a> | <a href=https://github.com/taodeng/CDNN-traffic-saliency>link</a></summary>
<ul>
Description: 16 videos of driving scenes with gaze data of 28 subjects recorded in the lab with eye-tracker
</ul>
</summary>
<ul>
Data: eye-tracking, scene video
</ul>
<ul>
<pre>
@article{2020_T-ITS_Deng,
    author = "Deng, Tao and Yan, Hongmei and Qin, Long and Ngo, Thuyen and Manjunath, BS",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "5",
    pages = "2146--2154",
    publisher = "IEEE",
    title = "{How do drivers allocate their potential attention? Driving fixation prediction via convolutional neural networks}",
    volume = "21",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="NeuroIV"></a>
<details close>
<summary>NeuroIV | <a href=https://doi.org/10.1109/TITS.2020.3022921>paper</a> | <a href=https://github.com/ispc-lab/NeuroIV>link</a></summary>
<ul>
Full name: Neuromorphic Vision Meets Intelligent Vehicle
</ul>
</summary>
<ul>
Description: Videos of drivers performing secondary tasks, making hand gestures and observing different regions inside the vehicle recorded with DAVIS and depth sensor
</ul>
</summary>
<ul>
Data: driver video
</ul>
<ul>
<pre>
@article{2020_T-ITS_Chen,
    author = {Chen, Guang and Wang, Fa and Li, Weijun and Hong, Lin and Conradt, J{\"o}rg and Chen, Jieneng and Zhang, Zhenyan and Lu, Yiwen and Knoll, Alois},
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "2",
    pages = "1171--1183",
    publisher = "IEEE",
    title = "NeuroIV: Neuromorphic vision meets intelligent vehicle towards safe driving with a new database and baseline evaluations",
    volume = "23",
    year = "2020"
}
</pre>
</details>
</ul>

<a name="LISA v2"></a>
<details close>
<summary>LISA v2 | <a href=https://doi.org/10.1109/IV47402.2020.9304573>paper</a> | <a href=https://github.com/arangesh/GPCycleGAN>link</a></summary>
<ul>
Full name: Laboratory for Intelligent and Safe Automobiles
</ul>
</summary>
<ul>
Description: Videos of drivers with and without eyeglasses recorded under different lighting conditions
</ul>
</summary>
<ul>
Data: driver video
</ul>
<ul>
<pre>
@inproceedings{2020_IV_Rangesh,
    author = "Rangesh, Akshay and Zhang, Bowen and Trivedi, Mohan M",
    booktitle = "IV",
    title = "Driver gaze estimation in the real world: Overcoming the eyeglass challenge",
    year = "2020"
}
</pre>
</details>
</ul>

<a name="DGAZE"></a>
<details close>
<summary>DGAZE | <a href=http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/ConferencePapers/2020/DGAZE_Driver.pdf>paper</a> | <a href=https://github.com/duaisha/DGAZE>link</a></summary>
<ul>
Description: A dataset mapping drivers’ gaze to different areas in a static traffic scene in lab conditions
</ul>
</summary>
<ul>
Data: driver video, scene video
</ul>
</summary>
<ul>
Annotations: bounding boxes
</ul>
<ul>
<pre>
@inproceedings{2020_IROS_Dua,
    author = "Dua, Isha and John, Thrupthi Ann and Gupta, Riya and Jawahar, CV",
    booktitle = "IROS",
    title = "DGAZE: Driver Gaze Mapping on Road",
    year = "2020"
}
</pre>
</details>
</ul>

<a name="DMD"></a>
<details close>
<summary>DMD | <a href=https://doi.org/10.1007/978-3-030-66823-5_23>paper</a> | <a href=https://dmd.vicomtech.org/>link</a></summary>
<ul>
Full name: Driving Monitoring Dataset
</ul>
</summary>
<ul>
Description: A diverse multi-modal dataset of drivers performing various secondary tasks, observing different regions inside the car, and showing signs of drowsiness recorded on-road and in simulation environment
</ul>
</summary>
<ul>
Data: driver video, scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: bounding boxes, action labels
</ul>
<ul>
<pre>
@inproceedings{2020_ECCVW_Ortega,
    author = "Ortega, Juan Diego and Kose, Neslihan and Ca{\\textasciitilde n}as, Paola and Chao, Min-An and Unnervik, Alexander and Nieto, Marcos and Otaegui, Oihana and Salgado, Luis",
    booktitle = "ECCV",
    title = "Dmd: A large-scale multi-modal driver monitoring dataset for attention and alertness analysis",
    year = "2020"
}
</pre>
</details>
</ul>

<a name="PRORETA 4"></a>
<details close>
<summary>PRORETA 4 | <a href=https://doi.org/10.1109/IVS.2019.8814224>paper</a> | <a href=https://www.proreta.tu-darmstadt.de/proreta_1_4/proreta4_1/datasets_1/index.en.jsp>link</a></summary>
<ul>
Description: Videos of traffic scenes recorded in instrumented vehicle with driver’s gaze data for evaluating accuracy of detecting driver’s current object of fixation
</ul>
</summary>
<ul>
Data: eye-tracking, driver video, scene video
</ul>
<ul>
<pre>
@inproceedings{2019_IV_Schwehr,
    author = "Schwehr, Julian and Knaust, Moritz and Willert, Volker",
    booktitle = "IV",
    title = "How to evaluate object-of-fixation detection",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="DADA-2000"></a>
<details close>
<summary>DADA-2000 | <a href=https://doi.org/10.1109/ITSC.2019.8917218>paper</a> | <a href=https://github.com/JWFangit/LOTVS-DADA>link</a></summary>
<ul>
Full name: Driver Attention in Driving Accident Scenarios
</ul>
</summary>
<ul>
Description: 2000 videos of accident videos collected from video hosting websites with eye-tracking data from 20 subjects collected in the lab.
</ul>
</summary>
<ul>
Data: eye-tracking, scene video
</ul>
</summary>
<ul>
Annotations: bounding boxes, accident category labels
</ul>
<ul>
<pre>
@inproceedings{2019_ITSC_Fang,
    author = "Fang, Jianwu and Yan, Dingxin and Qiao, Jiahuan and Xue, Jianru and Wang, He and Li, Sen",
    booktitle = "ITSC",
    title = "{DADA-2000: Can Driving Accident be Predicted by Driver Attentionƒ Analyzed by A Benchmark}",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="Drive&Act"></a>
<details close>
<summary>Drive&Act | <a href=https://openaccess.thecvf.com/content_ICCV_2019/papers/Martin_DriveAct_A_Multi-Modal_Dataset_for_Fine-Grained_Driver_Behavior_Recognition_in_ICCV_2019_paper.pdf>paper</a> | <a href=https://www.driveandact.com/>link</a></summary>
<ul>
Description: Videos of drivers performing various driving- and non-driving-related tasks
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: semantic maps, action labels
</ul>
<ul>
<pre>
@inproceedings{2019_ICCV_Martin,
    author = "Martin, Manuel and Roitberg, Alina and Haurilet, Monica and Horne, Matthias and Rei{\ss}, Simon and Voit, Michael and Stiefelhagen, Rainer",
    booktitle = "ICCV",
    title = "Drive\\&act: A multi-modal dataset for fine-grained driver behavior recognition in autonomous vehicles",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="RLDD"></a>
<details close>
<summary>RLDD | <a href=https://openaccess.thecvf.com/content_CVPRW_2019/papers/AMFG/Ghoddoosian_A_Realistic_Dataset_and_Baseline_Temporal_Model_for_Early_Drowsiness_CVPRW_2019_paper.pdf>paper</a> | <a href=https://github.com/rezaghoddoosian/Early-Drowsiness-Detection>link</a></summary>
<ul>
Full name: Real-Life Drowsiness Datase
</ul>
</summary>
<ul>
Description: Crowdsourced videos of people in various states of drowsiness recorded in indoor environments
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: drowsiness labels
</ul>
<ul>
<pre>
@inproceedings{2019_CVPRW_Ghoddoosian,
    author = "Ghoddoosian, Reza and Galib, Marnim and Athitsos, Vassilis",
    booktitle = "CVPRW",
    title = "A realistic dataset and baseline temporal model for early drowsiness detection",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="HAD"></a>
<details close>
<summary>HAD | <a href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Grounding_Human-To-Vehicle_Advice_for_Self-Driving_Vehicles_CVPR_2019_paper.pdf>paper</a> | <a href=https://usa.honda-ri.com/HAD>link</a></summary>
<ul>
Full name: HAD HRI Advice Dataset
</ul>
</summary>
<ul>
Description: A subset of videos from HDD naturalistic dataset annotated with textual advice containing 1) goals – where the vehicle should move and 2) attention – where the vehicle should look
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: goal and attention labels
</ul>
<ul>
<pre>
@inproceedings{2019_CVPR_Kim,
    author = "Kim, Jinkyu and Misu, Teruhisa and Chen, Yi-Ting and Tawari, Ashish and Canny, John",
    booktitle = "CVPR",
    title = "Grounding human-to-vehicle advice for self-driving vehicles",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="3MDAD"></a>
<details close>
<summary>3MDAD | <a href=https://doi.org/10.1007/978-3-030-29888-3_42>paper</a> | <a href=https://sites.google.com/site/benkhalifaanouar1/6-datasets>link</a></summary>
<ul>
Full name: Multimodal Multiview and Multispectral Driver Action Dataset
</ul>
</summary>
<ul>
Description: Videos of drivers performing secondary tasks
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: action labels, bounding boxes
</ul>
<ul>
<pre>
@inproceedings{2019_CAIP_Jegham,
    author = "Jegham, Imen and Ben Khalifa, Anouar and Alouani, Ihsen and Mahjoub, Mohamed Ali",
    booktitle = "Computer Analysis of Images and Patterns: 18th International Conference, CAIP 2019, Salerno, Italy, September 3--5, 2019, Proceedings, Part I 18",
    organization = "Springer",
    pages = "518--529",
    title = "Mdad: A multimodal and multiview in-vehicle driver action dataset",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="EBDD"></a>
<details close>
<summary>EBDD | <a href=https://doi.org/10.1109/TCSVT.2018.2818407>paper</a> | <a href=https://mahbubur.buet.ac.bd/resources/ebdd_database.htm>link</a></summary>
<ul>
Full name: EEE BUET Distracted Driving Dataset
</ul>
</summary>
<ul>
Description: Videos of drivers performing secondary tasks
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: action labels, bounding boxes
</ul>
<ul>
<pre>
@article{2019_TCSVT_Billah,
    author = "Billah, Tashrif and Rahman, SM Mahbubur and Ahmad, M Omair and Swamy, MNS",
    journal = "IEEE Transactions on Circuits and Systems for Video Technology",
    number = "4",
    pages = "1048--1062",
    publisher = "IEEE",
    title = "Recognizing distractions for assistive driving by tracking body parts",
    volume = "29",
    year = "2018"
}
</pre>
</details>
</ul>

<a name="H3D"></a>
<details close>
<summary>H3D | <a href=https://doi.org/10.1109/ICRA.2019.8793925>paper</a> | <a href=https://doi.org/10.1109/ICRA.2019.8793925>link</a></summary>
<ul>
Full name: H3D Honda 3D Dataset
</ul>
</summary>
<ul>
Description: A subset of videos from HDD dataset with 3D bounding boxes and object ids for tracking
</ul>
</summary>
<ul>
Data: driver video, vehicle data
</ul>
</summary>
<ul>
Annotations: bounding boxes
</ul>
<ul>
<pre>
@inproceedings{2019_ICRA_Patil,
    author = "Patil, Abhishek and Malla, Srikanth and Gang, Haiming and Chen, Yi-Ting",
    booktitle = "2019 International Conference on Robotics and Automation (ICRA)",
    organization = "IEEE",
    pages = "9552--9557",
    title = "The h3d dataset for full-surround 3d multi-object detection and tracking in crowded urban scenes",
    year = "2019"
}
</pre>
</details>
</ul>

<a name="DR(eye)VE"></a>
<details close>
<summary>DR(eye)VE | <a href=https://doi.org/10.1109/TPAMI.2018.2845370>paper</a> | <a href=http://imagelab.ing.unimore.it/dreyeve>link</a></summary>
<ul>
Description: Driving videos recorded on-road with corresponding gaze data of the driver
</ul>
</summary>
<ul>
Data: eye-tracking, scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: weather and road type labels
</ul>
<ul>
<pre>
@article{2018_PAMI_Palazzi,
    author = "Palazzi, Andrea and Abati, Davide and Solera, Francesco and Cucchiara, Rita and others",
    journal = "IEEE TPAMI",
    number = "7",
    pages = "1720--1733",
    title = "{Predicting the Driver's Focus of Attention: the DR (eye) VE Project}",
    volume = "41",
    year = "2018"
}
</pre>
</details>
</ul>

<a name="BDD-X"></a>
<details close>
<summary>BDD-X | <a href=https://openaccess.thecvf.com/content_ECCV_2018/papers/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.pdf>paper</a> | <a href=https://github.com/JinkyuKimUCB/BDD-X-dataset>link</a></summary>
<ul>
Full name: Berkeley Deep Drive-X (eXplanation) Dataset
</ul>
</summary>
<ul>
Description: A subset of videos from BDD dataset annotated with textual descriptions of actions performed by the vehicle and explanations justifying those actions
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: action explanations
</ul>
<ul>
<pre>
@inproceedings{2018_ECCV_Kim,
    author = "Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep",
    booktitle = "ECCV",
    title = "Textual explanations for self-driving vehicles",
    year = "2018"
}
</pre>
</details>
</ul>

<a name="HDD"></a>
<details close>
<summary>HDD | <a href=https://openaccess.thecvf.com/content_cvpr_2018/papers/Ramanishka_Toward_Driving_Scene_CVPR_2018_paper.pdf>paper</a> | <a href=https://usa.honda-ri.com/HDD>link</a></summary>
<ul>
Full name: HDD HRI Driving Dataset
</ul>
</summary>
<ul>
Description: A large naturalistic driving dataset with driving footage, vehicle telemetry and annotations for vehicle actions and their justifications
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: bounding boxes, action labels
</ul>
<ul>
<pre>
@inproceedings{2018_CVPR_Ramanishka,
    author = "Ramanishka, Vasili and Chen, Yi-Ting and Misu, Teruhisa and Saenko, Kate",
    booktitle = "CVPR",
    title = "Toward driving scene understanding: A dataset for learning driver behavior and causal reasoning",
    year = "2018"
}
</pre>
</details>
</ul>

<a name="BDD-A"></a>
<details close>
<summary>BDD-A | <a href=https://doi.org/10.1007/978-3-030-20873-8_42>paper</a> | <a href=https://bdd-data.berkeley.edu/>link</a></summary>
<ul>
Full name: Berkeley Deep Drive-A (Attention) Dataset
</ul>
</summary>
<ul>
Description: A set of short video clips extracted from the Berkeley Deep Drive (BDD) dataset with additional eye-tracking data collected in the lab from 45 subjects
</ul>
</summary>
<ul>
Data: eye-tracking, scene video, vehicle data
</ul>
<ul>
<pre>
@inproceedings{2018_ACCV_Xia,
    author = "Xia, Ye and Zhang, Danqing and Kim, Jinkyu and Nakayama, Ken and Zipser, Karl and Whitney, David",
    booktitle = "ACCV",
    title = "Predicting driver attention in critical situations",
    year = "2018"
}
</pre>
</details>
</ul>

<a name="C42CN"></a>
<details close>
<summary>C42CN | <a href=https://doi.org/10.1038/sdata.2017.110>paper</a> | <a href=https://osf.io/c42cn/>link</a></summary>
<ul>
Description: A multi-modal dataset acquired in a controlled experiment on a driving simulator under 4 conditions: no distraction, cognitive, emotional and sensorimotor distraction.
</ul>
</summary>
<ul>
Data: eye-tracking, scene video, physiological signal
</ul>
<ul>
<pre>
@article{2017_NatSciData_Taamneh,
    author = "Taamneh, Salah and Tsiamyrtzis, Panagiotis and Dcosta, Malcolm and Buddharaju, Pradeep and Khatri, Ashik and Manser, Michael and Ferris, Thomas and Wunderlich, Robert and Pavlidis, Ioannis",
    journal = "Scientific Data",
    pages = "170110",
    title = "A multimodal dataset for various forms of distracted driving",
    volume = "4",
    year = "2017"
}
</pre>
</details>
</ul>

<a name="DriveAHead"></a>
<details close>
<summary>DriveAHead | <a href=https://openaccess.thecvf.com/content_cvpr_2017_workshops/w13/papers/Schwarz_DriveAHead_-_A_CVPR_2017_paper.pdf>paper</a> | <a href=https://cvhci.anthropomatik.kit.edu/data/DriveAHead/>link</a></summary>
<ul>
Description: Videos of drivers with frame-level head pose annotations obtained from a motion-capture system
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: occlusion, head pose, depth
</ul>
<ul>
<pre>
@inproceedings{2017_CVPRW_Schwarz,
    author = "Schwarz, Anke and Haurilet, Monica and Martinez, Manuel and Stiefelhagen, Rainer",
    booktitle = "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops",
    pages = "1--10",
    title = "Driveahead-a large-scale driver head pose dataset",
    year = "2017"
}
</pre>
</details>
</ul>

<a name="DDD"></a>
<details close>
<summary>DDD | <a href=https://doi.org/10.1007/978-3-319-54526-4_9>paper</a> | <a href=http://cv.cs.nthu.edu.tw/php/callforpaper/datasets/DDD/>link</a></summary>
<ul>
Full name: Driver Drowsiness Detection Dataset
</ul>
</summary>
<ul>
Description: Videos of human subjects simulating different levels of drowsiness while driving in a simulator
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: drowsiness labels
</ul>
<ul>
<pre>
@inproceedings{2017_ACCV_Weng,
    author = "Weng, Ching-Hua and Lai, Ying-Hsiu and Lai, Shang-Hong",
    booktitle = "ACCV",
    title = "Driver drowsiness detection via a hierarchical temporal deep belief network",
    year = "2016"
}
</pre>
</details>
</ul>

<a name="Dashcam dataset"></a>
<details close>
<summary>Dashcam dataset | <a href=https://github.com/SullyChen/driving-datasets>link</a></summary>
<ul>
Description: Driving videos with steering information recorded on road
</ul>
</summary>
<ul>
Data: scene video
</ul>
<ul>
<pre>
</pre>
</details>
</ul>

<a name="AUCD2"></a>
<details close>
<summary>AUCD2 | <a href=https://arxiv.org/pdf/1706.09498.pdf>paper</a> | <a href=https://abouelnaga.io/projects/auc-distracted-driver-dataset/>link</a></summary>
<ul>
Full name: American University in Cairo (AUC) Distracted Driver’s Dataset
</ul>
</summary>
<ul>
Description: Videos of drivers performing secondary tasks
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: action labels
</ul>
<ul>
<pre>
@inproceedings{2017_NeurIPS_Abouelnaga,
    author = "Abouelnaga, Yehya and Eraqi, Hesham M. and Moustafa, Mohamed N.",
    booktitle = "NeurIPS Workshop on Machine Learning for Intelligent Transportation Systems",
    title = "eal-time Distracted Driver Posture Classification",
    year = "2017"
}
</pre>
</details>
</ul>

<a name="DROZY"></a>
<details close>
<summary>DROZY | <a href=https://doi.org/10.1109/WACV.2016.7477715>paper</a> | <a href=http://www.drozy.ulg.ac.be/>link</a></summary>
<ul>
Description: Videos and physiological data from subjects in different drowsiness states after prolonged waking
</ul>
</summary>
<ul>
Data: driver video, physiological signal
</ul>
</summary>
<ul>
Annotations: drowsiness labels
</ul>
<ul>
<pre>
@inproceedings{2016_WACV_Massoz,
    author = "Massoz, Quentin and Langohr, Thomas and Fran{\c{c}}ois, Cl{\'e}mentine and Verly, Jacques G",
    booktitle = "WACV",
    title = "The ULg multimodality drowsiness database (called DROZY) and examples of use",
    year = "2016"
}
</pre>
</details>
</ul>

<a name="TETD"></a>
<details close>
<summary>TETD | <a href=https://doi.org/10.1109/TITS.2016.2535402>paper</a> | <a href=https://github.com/taodeng/traffic-eye-tracking-dataset>link</a></summary>
<ul>
Full name: Traffic Eye Tracking Dataset
</ul>
</summary>
<ul>
Description: A set of 100 images of traffic scenes with corresponding eye-tracking data from 20 subjects
</ul>
</summary>
<ul>
Data: eye-tracking, scene images
</ul>
<ul>
<pre>
@article{2016_T-ITS_Deng,
    author = "Deng, Tao and Yang, Kaifu and Li, Yongjie and Yan, Hongmei",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "7",
    pages = "2051--2062",
    publisher = "IEEE",
    title = "Where does the driver look? Top-down-based saliency detection in a traffic driving environment",
    volume = "17",
    year = "2016"
}
</pre>
</details>
</ul>

<a name="DAD"></a>
<details close>
<summary>DAD | <a href=https://doi.org/10.1007/978-3-319-54190-7_9>paper</a> | <a href=https://aliensunmin.github.io/project/dashcam/>link</a></summary>
<ul>
Description: Videos of accidents recorded with dashboard cameras sourced from video hosting sites with annotations for accidents and road users involved in them
</ul>
</summary>
<ul>
Data: scene video
</ul>
</summary>
<ul>
Annotations: bounding boxes, accident category labels
</ul>
<ul>
<pre>
@inproceedings{2016_ACCV_Chan,
    author = "Chan, Fu-Hsiang and Chen, Yu-Ting and Xiang, Yu and Sun, Min",
    booktitle = "ACCV",
    title = "Anticipating accidents in dashcam videos",
    year = "2016"
}
</pre>
</details>
</ul>

<a name="Brain4Cars"></a>
<details close>
<summary>Brain4Cars | <a href=https://openaccess.thecvf.com/content_iccv_2015/papers/Jain_Car_That_Knows_ICCV_2015_paper.pdf>paper</a> | <a href=https://github.com/asheshjain399/ICCV2015_Brain4Cars>link</a></summary>
<ul>
Description: Synchronized videos from scene and driver-facing cameras of drivers performing various maneuvers in traffic
</ul>
</summary>
<ul>
Data: driver video, scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: action labels
</ul>
<ul>
<pre>
@inproceedings{2015_ICCV_Jain,
    author = "Jain, Ashesh and Koppula, Hema S and Raghavan, Bharad and Soh, Shane and Saxena, Ashutosh",
    booktitle = "ICCV",
    title = "Car that knows before you do: Anticipating maneuvers via learning temporal driving models",
    year = "2015"
}
</pre>
</details>
</ul>

<a name="SFD"></a>
<details close>
<summary>SFD | <a href=https://www.kaggle.com/competitions/state-farm-distracted-driver-detection/overview>link</a></summary>
<ul>
Full name: State Farm Distracted Driver Detection
</ul>
</summary>
<ul>
Description: Videos of drivers performing secondary tasks
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: action labels
</ul>
<ul>
<pre>
</pre>
</details>
</ul>

<a name="DIPLECS Surrey"></a>
<details close>
<summary>DIPLECS Surrey | <a href=https://doi.org/10.1109/TVT.2015.2487826>paper</a> | <a href=https://cvssp.org/data/diplecs/>link</a></summary>
<ul>
Description: Driving videos with steering information recorded in different cars and environments
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
<ul>
<pre>
@article{2015_TranVehTech_Pugeault,
    author = "Pugeault, Nicolas and Bowden, Richard",
    journal = "IEEE Transactions on Vehicular Technology",
    number = "12",
    pages = "5424--5438",
    publisher = "IEEE",
    title = "How much of driving is preattentive?",
    volume = "64",
    year = "2015"
}
</pre>
</details>
</ul>

<a name="YawDD"></a>
<details close>
<summary>YawDD | <a href=https://doi.org/10.1145/2557642.2563678>paper</a> | <a href=https://ieee-dataport.org/open-access/yawdd-yawning-detection-dataset>link</a></summary>
<ul>
Full name: Yawning Detection Dataset
</ul>
</summary>
<ul>
Description: Recordings of human subjects in parked vehicles simulating normal driving, singing and taslking, and yawning
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: bounding boxes, action labels
</ul>
<ul>
<pre>
@inproceedings{2014_ACM_Abtahi,
    author = "Abtahi, Shabnam and Omidyeganeh, Mona and Shirmohammadi, Shervin and Hariri, Behnoosh",
    booktitle = "Proceedings of the ACM Multimedia Systems Conference",
    title = "{YawDD: A yawning detection dataset}",
    year = "2014"
}
</pre>
</details>
</ul>

<a name="3DDS"></a>
<details close>
<summary>3DDS | <a href=http://www.bmva.org/bmvc/2011/proceedings/paper85/paper85.pdf>paper</a> | <a href=http://ilab.usc.edu/borji/Resources.html>link</a></summary>
<ul>
Full name: 3D Driving School Dataset
</ul>
</summary>
<ul>
Description: Videos and eye-tracking data of people playing 3D driving simulator game
</ul>
</summary>
<ul>
Data: eye-tracking, scene video
</ul>
<ul>
<pre>
@inproceedings{2011_BMVC_Borji,
    author = "Borji, Ali and Sihite, Dicky N and Itti, Laurent",
    booktitle = "BMVC",
    title = "Computational Modeling of Top-down Visual Attention in Interactive Environments.",
    year = "2011"
}
</pre>
</details>
</ul>

<a name="DIPLECS Sweden"></a>
<details close>
<summary>DIPLECS Sweden | <a href=https://doi.org/10.1007/978-3-642-15567-3_12>paper</a> | <a href=https://cvssp.org/data/diplecs/>link</a></summary>
<ul>
Description: Driving videos with steering information recorded in different cars and environments
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
<ul>
<pre>
@inproceedings{2010_ACCV_Pugeault,
    author = "Pugeault, Nicolas and Bowden, Richard",
    booktitle = "ECCV",
    title = "Learning pre-attentive driving behaviour from holistic visual features",
    year = "2010"
}
</pre>
</details>
</ul>

<a name="BU HeadTracking"></a>
<details close>
<summary>BU HeadTracking | <a href=https://doi.org/10.1109/34.845375>paper</a> | <a href=https://www.cs.bu.edu/groups/ivc/HeadTracking/>link</a></summary>
<ul>
Full name: Boston University Head Tracking Dataset
</ul>
</summary>
<ul>
Description: Videos and head tracking information for multiple human subjects recorded in diverse conditions
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: head pose
</ul>
<ul>
<pre>
@article{2000_PAMI_LaCascia,
    author = "La Cascia, Marco and Sclaroff, Stan and Athitsos, Vassilis",
    journal = "IEEE Transactions on pattern analysis and machine intelligence",
    number = "4",
    pages = "322--336",
    publisher = "IEEE",
    title = "Fast, reliable head tracking under varying illumination: An approach based on registration of texture-mapped 3D models",
    volume = "22",
    year = "2000"
}
</pre>
</details>
</ul>

