---
<a href=README.md/#top><l style="font-size:30px">Home</l></a>&nbsp;&nbsp;| <a href=behavioral.md><l style="font-size:30px">Behavioral</l></a>&nbsp;&nbsp;| <l style="font-size:35px">Applications</l>&nbsp;&nbsp;| <a href=datasets.md><l style="font-size:30px">Datasets</l></a>&nbsp;&nbsp;
---

Scene gaze&nbsp;&nbsp;| [In-vehicle gaze](in-vehicle_gaze.md)&nbsp;&nbsp;| [Distraction detection](distraction_detection.md)&nbsp;&nbsp;| [Drowsiness detection](drowsiness_detection.md)&nbsp;&nbsp;| [Action anticipation](action_anticipation.md)&nbsp;&nbsp;| [Driver awareness](driver_awareness.md)&nbsp;&nbsp;| [Self-driving](self-driving.md)&nbsp;&nbsp;| [Papers with code](papers_with_code.md)&nbsp;&nbsp;
___
*Click on each entry below to see additional information.*
<ul><a name=2023_ICCV_Chen></a>
<details close>
<summary>Chen et al., FBLNet: FeedBack Loop Network for Driver Attention Prediction, ICCV, 2023 | <a href=https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_FBLNet_FeedBack_Loop_Network_for_Driver_Attention_Prediction_ICCV_2023_paper.pdf>paper</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-A>BDD-A</a>, <a href=datasets.md#DADA-2000>DADA-2000</a>
</ul>
<ul>
<pre>
@inproceedings{2023_ICCV_Chen,
    author = "Chen, Yilong and Nan, Zhixiong and Xiang, Tao",
    booktitle = "Proceedings of the IEEE/CVF International Conference on Computer Vision",
    pages = "13371--13380",
    title = "FBLNet: FeedBack Loop Network for Driver Attention Prediction",
    year = "2023"
}
</pre>
</ul>
</ul>
<ul><a name=2023_IV_Biswas></a>
<details close>
<summary>Biswas et al., Characterizing Drivers’ Peripheral Vision via the Functional Field of View for Intelligent Driving Assistance, IV, 2023 | <a href=https://doi.org/10.1109/IV55152.2023.10186746>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@inproceedings{2023_IV_Biswas,
    author = "Biswas, Abhijat and Admoni, Henny",
    booktitle = "2023 IEEE Intelligent Vehicles Symposium (IV)",
    organization = "IEEE",
    pages = "1--8",
    title = "Characterizing Drivers’ Peripheral Vision via the Functional Field of View for Intelligent Driving Assistance",
    year = "2023"
}
</pre>
</ul>
</ul>
<ul><a name=2023_IV_Bhagat></a>
<details close>
<summary>Bhagat et al., Driver Gaze Fixation and Pattern Analysis in Safety Critical Events, IV, 2023 | <a href=https://doi.org/10.1109/IV55152.2023.10186718>paper</a> | <a href=https://github.com/VTTI/gaze-fixation-and-object-saliency>code</a></summary>
<ul>
Dataset(s): SHRP2
</ul>
<ul>
<pre>
@inproceedings{2023_IV_Bhagat,
    author = "Bhagat, Hirva and Jain, Sandesh and Abbott, Lynn and Sonth, Akash and Sarkar, Abhijit",
    booktitle = "2023 IEEE Intelligent Vehicles Symposium (IV)",
    organization = "IEEE",
    pages = "1--8",
    title = "Driver gaze fixation and pattern analysis in safety critical events",
    year = "2023"
}
</pre>
</ul>
</ul>
<ul><a name=2022_T-ITS_Qin></a>
<details close>
<summary>Qin et al., ID-YOLO: Real-Time Salient Object Detection Based on the Driver’s Fixation Region, Trans. ITS, 2022 | <a href=https://doi.org/10.1109/TITS.2022.3146271>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@article{2022_T-ITS_Qin,
    author = "Qin, Long and Shi, Yi and He, Yahui and Zhang, Junrui and Zhang, Xianshi and Li, Yongjie and Deng, Tao and Yan, Hongmei",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "9",
    pages = "15898--15908",
    publisher = "IEEE",
    title = "ID-YOLO: Real-time salient object detection based on the driver’s fixation region",
    volume = "23",
    year = "2022"
}
</pre>
</ul>
</ul>
<ul><a name=2022_T-ITS_Li></a>
<details close>
<summary>Li et al., Adaptive Short-Temporal Induced Aware Fusion Network for Predicting Attention Regions Like a Driver, Trans. ITS, 2022 | <a href=https://doi.org/10.1109/TITS.2022.3165619>paper</a> | <a href=https://github.com/liuchunsense/ASIAFnet>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-A>BDD-A</a>, <a href=datasets.md#DADA-2000>DADA-2000</a>, <a href=datasets.md#TrafficSaliency>TrafficSaliency</a>
</ul>
<ul>
<pre>
@article{2022_T-ITS_Li,
    author = "Li, Qiang and Liu, Chunsheng and Chang, Faliang and Li, Shuang and Liu, Hui and Liu, Zehao",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "10",
    pages = "18695--18706",
    publisher = "IEEE",
    title = "Adaptive short-temporal induced aware fusion network for predicting attention regions like a driver",
    volume = "23",
    year = "2022"
}
</pre>
</ul>
</ul>
<ul><a name=2022_T-ITS_Gan></a>
<details close>
<summary>Gan et al., Multisource Adaption for Driver Attention Prediction in Arbitrary Driving Scenes, Trans. ITS, 2022 | <a href=https://doi.org/10.1109/TITS.2022.3177640>paper</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-A>BDD-A</a>, <a href=datasets.md#DADA-2000>DADA-2000</a>, <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>, <a href=datasets.md#TrafficSaliency>TrafficSaliency</a>
</ul>
<ul>
<pre>
@article{2022_T-ITS_Gan,
    author = "Gan, Shun and Pei, Xizhe and Ge, Yulong and Wang, Qingfan and Shang, Shi and Li, Shengbo Eben and Nie, Bingbing",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "11",
    pages = "20912--20925",
    publisher = "IEEE",
    title = "Multisource Adaption for Driver Attention Prediction in Arbitrary Driving Scenes",
    volume = "23",
    year = "2022"
}
</pre>
</ul>
</ul>
<ul><a name=2022_IV_Araluce></a>
<details close>
<summary>Araluce et al., ARAGAN: A dRiver Attention estimation model based on conditional Generative Adversarial Network, IV, 2022 | <a href=https://doi.org/10.1109/IV51971.2022.9827175>paper</a> | <a href=https://github.com/javierAraluce/ARAGAN>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-A>BDD-A</a>, <a href=datasets.md#DADA-2000>DADA-2000</a>
</ul>
<ul>
<pre>
@inproceedings{2022_IV_Araluce,
    author = "Araluce, Javier and Bergasa, Luis M and Oca{\\textasciitilde n}a, Manuel and Barea, Rafael and L{\'o}pez-Guill{\'e}n, Elena and Revenga, Pedro",
    booktitle = "2022 IEEE Intelligent Vehicles Symposium (IV)",
    organization = "IEEE",
    pages = "1066--1072",
    title = "ARAGAN: A dRiver Attention estimation model based on conditional Generative Adversarial Network",
    year = "2022"
}
</pre>
</ul>
</ul>
<ul><a name=2022_ECCV_Kasahara></a>
<details close>
<summary>Kasahara et al., Look Both Ways: Self-Supervising Driver Gaze Estimation and Road Scene Saliency, ECCV, 2022 | <a href=https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136730128.pdf>paper</a> | <a href=https://github.com/Kasai2020/look_both_ways>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#LBW>LBW</a>
</ul>
<ul>
<pre>
@inproceedings{2022_ECCV_Kasahara,
    author = "Kasahara, Isaac and Stent, Simon and Park, Hyun Soo",
    booktitle = "Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XIII",
    organization = "Springer",
    pages = "126--142",
    title = "Look Both Ways: Self-supervising Driver Gaze Estimation and Road Scene Saliency",
    year = "2022"
}
</pre>
</ul>
</ul>
<ul><a name=2022_T-ITS_Fang></a>
<details close>
<summary>Fang et al., DADA: Driver Attention Prediction in Driving Accident Scenarios, Trans. ITS, 2021 | <a href=https://doi.org/10.1109/TITS.2020.3044678>paper</a> | <a href=https://github.com/JWFangit/LOTVS-DADA>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#TrafficSaliency>TrafficSaliency</a>, <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>, <a href=datasets.md#DADA-2000>DADA-2000</a>
</ul>
<ul>
<pre>
@article{2022_T-ITS_Fang,
    author = "Fang, Jianwu and Yan, Dingxin and Qiao, Jiahuan and Xue, Jianru and Yu, Hongkai",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "6",
    pages = "4959--4971",
    publisher = "IEEE",
    title = "DADA: Driver attention prediction in driving accident scenarios",
    volume = "23",
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2021_T-ITS_Amadori></a>
<details close>
<summary>Amadori et al., HammerDrive: A Task-Aware Driving Visual Attention Model, Trans. ITS, 2021 | <a href=https://doi.org/10.1109/TITS.2021.3055120>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@article{2021_T-ITS_Amadori,
    author = "Amadori, Pierluigi Vito and Fischer, Tobias and Demiris, Yiannis",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    title = "HammerDrive: A Task-Aware Driving Visual Attention Model",
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2021_IV_Epple></a>
<details close>
<summary>Epple et al., How Do Drivers Observe Surrounding Vehicles in Real-World Traffic? Estimating the Drivers Primary Observed Traffic Objects, IV, 2021 | <a href=https://doi.org/10.1109/IV48863.2021.9575202>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@inproceedings{2021_IV_Epple,
    author = "Epple, Nico and Chopra, Harshit and Riener, Andreas",
    booktitle = "2021 IEEE Intelligent Vehicles Symposium (IV)",
    organization = "IEEE",
    pages = "594--601",
    title = "How Do Drivers Observe Surrounding Vehicles in Real-World Traffic? Estimating the Drivers Primary Observed Traffic Objects",
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2021_ICCVW_Gopinath></a>
<details close>
<summary>Gopinath et al., MAAD: A Model and Dataset for “Attended Awareness” in Driving, ICCVW, 2021 | <a href=https://openaccess.thecvf.com/content/ICCV2021W/EPIC/papers/Gopinath_MAAD_A_Model_and_Dataset_for_Attended_Awareness_in_Driving_ICCVW_2021_paper.pdf>paper</a> | <a href=https://github.com/ToyotaResearchInstitute/att-aware/>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#MAAD>MAAD</a>
</ul>
<ul>
<pre>
@inproceedings{2021_ICCVW_Gopinath,
    author = "Gopinath, Deepak and Rosman, Guy and Stent, Simon and Terahata, Katsuya and Fletcher, Luke and Argall, Brenna and Leonard, John",
    booktitle = "Proceedings of the IEEE/CVF International Conference on Computer Vision",
    pages = "3426--3436",
    title = {MAAD: A Model and Dataset for" Attended Awareness" in Driving},
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2021_ICCV_Baee></a>
<details close>
<summary>Baee et al., MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning, ICCV, 2021 | <a href=https://openaccess.thecvf.com/content/ICCV2021/papers/Baee_MEDIRL_Predicting_the_Visual_Attention_of_Drivers_via_Maximum_Entropy_ICCV_2021_paper.pdf>paper</a> | <a href=https://github.com/soniabaee/MEDIRL-EyeCar>code</a></summary>
<ul>
Dataset(s): Eyecar
</ul>
<ul>
<pre>
@inproceedings{2021_ICCV_Baee,
    author = "Baee, Sonia and Pakdamanian, Erfan and Kim, Inki and Feng, Lu and Ordonez, Vicente and Barnes, Laura",
    booktitle = "ICCV",
    title = "MEDIRL: Predicting the visual attention of drivers via maximum entropy deep inverse reinforcement learning",
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2020_T-ITS_Deng></a>
<details close>
<summary>Deng et al., How Do Drivers Allocate Their Potential Attention? Driving Fixation Prediction via Convolutional Neural Networks, Trans. ITS, 2020 | <a href=https://doi.org/10.1109/TITS.2019.2915540>paper</a> | <a href=https://github.com/taodeng/CDNN-traffic-saliency>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#TrafficSaliency>TrafficSaliency</a>
</ul>
<ul>
<pre>
@article{2020_T-ITS_Deng,
    author = "Deng, Tao and Yan, Hongmei and Qin, Long and Ngo, Thuyen and Manjunath, BS",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "5",
    pages = "2146--2154",
    publisher = "IEEE",
    title = "{How do drivers allocate their potential attention? Driving fixation prediction via convolutional neural networks}",
    volume = "21",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2020_ICRA_Zhang></a>
<details close>
<summary>Zhang et al., Interaction Graphs for Object Importance Estimation in On-road Driving Videos, ICRA, 2020 | <a href=https://arxiv.org/pdf/2003.06045.pdf>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@inproceedings{2020_ICRA_Zhang,
    author = "Zhang, Zehua and Tawari, Ashish and Martin, Sujitha and Crandall, David",
    booktitle = "ICRA",
    title = "Interaction Graphs for Object Importance Estimation in On-road Driving Videos",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2020_CVPR_Pal></a>
<details close>
<summary>Pal et al., “Looking at the right stuff” - Guided semantic-gaze for autonomous driving, CVPR, 2020 | <a href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Pal_Looking_at_the_Right_Stuff_-_Guided_Semantic-Gaze_for_Autonomous_CVPR_2020_paper.pdf>paper</a> | <a href=https://sites.google.com/eng.ucsd.edu/sage-net>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>, <a href=datasets.md#BDD-A>BDD-A</a>, JAAD
</ul>
<ul>
<pre>
@inproceedings{2020_CVPR_Pal,
    author = "Pal, Anwesan and Mondal, Sayan and Christensen, Henrik I",
    booktitle = "CVPR",
    title = {{" Looking at the Right Stuff"-Guided Semantic-Gaze for Autonomous Driving}},
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2019_WACV_Tavakoli></a>
<details close>
<summary>Tavakoli et al., Digging Deeper into Egocentric Gaze Prediction, WACV, 2019 | <a href=https://doi.org/10.1109/WACV.2019.00035>paper</a></summary>
<ul>
Dataset(s): <a href=datasets.md#3DDS>3DDS</a>
</ul>
<ul>
<pre>
@inproceedings{2019_WACV_Tavakoli,
    author = "Tavakoli, Hamed Rezazadegan and Rahtu, Esa and Kannala, Juho and Borji, Ali",
    booktitle = "WACV",
    title = "Digging deeper into egocentric gaze prediction",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2019_IV_Rahimpour></a>
<details close>
<summary>Rahimpour et al., Context Aware Road-user Importance Estimation (iCARE), IV, 2019 | <a href=https://doi.org/10.1109/IVS.2019.8814210>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@inproceedings{2019_IV_Rahimpour,
    author = "Rahimpour, Alireza and Martin, Sujitha and Tawari, Ashish and Qi, Hairong",
    booktitle = "IV",
    title = "{Context Aware Road-user Importance Estimation (iCARE)}",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2019_ITSC_Ning></a>
<details close>
<summary>Ning et al., An Efficient Model for Driving Focus of Attention Prediction using Deep Learning, ITSC, 2019 | <a href=https://doi.org/10.1109/ITSC.2019.8917337>paper</a></summary>
<ul>
Dataset(s): <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>
</ul>
<ul>
<pre>
@inproceedings{2019_ITSC_Ning,
    author = "Ning, Minghao and Lu, Chao and Gong, Jianwei",
    booktitle = "ITCS",
    title = "{An Efficient Model for Driving Focus of Attention Prediction using Deep Learning}",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2019_ICRA_Gao></a>
<details close>
<summary>Gao et al., Goal-oriented object importance estimation in on-road driving videos, ICRA, 2019 | <a href=https://doi.org/10.1109/ICRA.2019.8793970>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@inproceedings{2019_ICRA_Gao,
    author = "Gao, Mingfei and Tawari, Ashish and Martin, Sujitha",
    booktitle = "ICRA",
    title = "Goal-oriented object importance estimation in on-road driving videos",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2019_BMVC_Palasek></a>
<details close>
<summary>Palasek et al., Attentional demand estimation with attentive driving models, BMVC, 2019 | <a href=https://bmvc2019.org/wp-content/uploads/papers/0996-paper.pdf>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@inproceedings{2019_BMVC_Palasek,
    author = "Palasek, Petar and Lavie, Nilli and Palmer, Luke",
    booktitle = "BMVC",
    title = "Attentional demand estimation with attentive driving models",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2018_T-ITS_Deng></a>
<details close>
<summary>Deng et al., Learning to Boost Bottom-Up Fixation Prediction in Driving Environments via Random Forest, Trans. ITS, 2018 | <a href=https://doi.org/10.1109/TITS.2017.2766216>paper</a></summary>
<ul>
Dataset(s): <a href=datasets.md#TETD>TETD</a>
</ul>
<ul>
<pre>
@article{2018_T-ITS_Deng,
    author = "Deng, Tao and Yan, Hongmei and Li, Yong-Jie",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "9",
    pages = "3059--3067",
    title = "Learning to boost bottom-up fixation prediction in driving environments via random forest",
    volume = "19",
    year = "2017"
}
</pre>
</ul>
</ul>
<ul><a name=2018_PAMI_Palazzi></a>
<details close>
<summary>Palazzi et al., Predicting the Driver’s Focus of Attention: the DR(eye)VE Project, PAMI, 2018 | <a href=https://doi.org/10.1109/TPAMI.2018.2845370>paper</a> | <a href=https://github.com/ndrplz/dreyeve>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>
</ul>
<ul>
<pre>
@article{2018_PAMI_Palazzi,
    author = "Palazzi, Andrea and Abati, Davide and Solera, Francesco and Cucchiara, Rita and others",
    journal = "IEEE TPAMI",
    number = "7",
    pages = "1720--1733",
    title = "{Predicting the Driver's Focus of Attention: the DR (eye) VE Project}",
    volume = "41",
    year = "2018"
}
</pre>
</ul>
</ul>
<ul><a name=2018_ITSC_Tawari></a>
<details close>
<summary>Tawari et al., Learning to Attend to Salient Targets in Driving Videos Using Fully Convolutional RNN, ITSC, 2018 | <a href=https://doi.org/10.1109/ITSC.2018.8569438>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@inproceedings{2018_ITSC_Tawari,
    author = "Tawari, Ashish and Mallela, Praneeta and Martin, Sujitha",
    booktitle = "ITSC",
    title = "Learning to attend to salient targets in driving videos using fully convolutional rnn",
    year = "2018"
}
</pre>
</ul>
</ul>
<ul><a name=2018_ACCV_Xia></a>
<details close>
<summary>Xia et al., Predicting Driver Attention in Critical Situations, ACCV, 2018 | <a href=https://doi.org/10.1007/978-3-030-20873-8_42>paper</a> | <a href=https://github.com/pascalxia/driver_attention_prediction>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-A>BDD-A</a>
</ul>
<ul>
<pre>
@inproceedings{2018_ACCV_Xia,
    author = "Xia, Ye and Zhang, Danqing and Kim, Jinkyu and Nakayama, Ken and Zipser, Karl and Whitney, David",
    booktitle = "ACCV",
    title = "Predicting driver attention in critical situations",
    year = "2018"
}
</pre>
</ul>
</ul>
<ul><a name=2017_WACV_Palmer></a>
<details close>
<summary>Palmer et al., Predicting the Perceptual Demands of Urban Driving with Video Regression, WACV, 2017 | <a href=https://doi.org/10.1109/WACV.2017.52>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@inproceedings{2017_WACV_Palmer,
    author = "Palmer, Luke and Bialkowski, Alina and Brostow, Gabriel J and Ambeck-Madsen, Jonas and Lavie, Nilli",
    booktitle = "WACV",
    title = "Predicting the perceptual demands of urban driving with video regression",
    year = "2017"
}
</pre>
</ul>
</ul>
<ul><a name=2017_PR_Ohn-Bar></a>
<details close>
<summary>Ohn-Bar et al., Are all objects equal? Deep spatio-temporal importance prediction in driving videos, Pattern Recognition, 2017 | <a href=https://doi.org/10.1016/j.patcog.2016.08.029>paper</a> | <a href=https://github.com/eshed1/Object_Importance>code</a></summary>
<ul>
Dataset(s): KITTI
</ul>
<ul>
<pre>
@article{2017_PR_Ohn-Bar,
    author = "Ohn-Bar, Eshed and Trivedi, Mohan Manubhai",
    journal = "Pattern Recognition",
    pages = "425--436",
    title = "Are all objects equal? Deep spatio-temporal importance prediction in driving videos",
    volume = "64",
    year = "2017"
}
</pre>
</ul>
</ul>
<ul><a name=2017_IV_Tawari></a>
<details close>
<summary>Tawari et al., A Computational Framework for Driver’s Visual Attention Using A Fully Convolutional Architecture, IV, 2017 | <a href=https://doi.org/10.1109/IVS.2017.7995828>paper</a></summary>
<ul>
Dataset(s): <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>
</ul>
<ul>
<pre>
@inproceedings{2017_IV_Tawari,
    author = "Tawari, Ashish and Kang, Byeongkeun",
    booktitle = "IV",
    title = "A computational framework for driver's visual attention using a fully convolutional architecture",
    year = "2017"
}
</pre>
</ul>
</ul>
<ul><a name=2017_IV_Palazzi></a>
<details close>
<summary>Palazzi et al., Learning Where to Attend Like a Human Driver, IV, 2017 | <a href=https://doi.org/10.1109/IVS.2017.7995833>paper</a> | <a href=https://github.com/francescosolera/dreyeving>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>
</ul>
<ul>
<pre>
@inproceedings{2017_IV_Palazzi,
    author = "Palazzi, Andrea and Solera, Francesco and Calderara, Simone and Alletto, Stefano and Cucchiara, Rita",
    booktitle = "IV",
    title = "Learning where to attend like a human driver",
    year = "2017"
}
</pre>
</ul>
</ul>
<ul><a name=2016_T-ITS_Deng></a>
<details close>
<summary>Deng et al., Where Does the Driver Look? Top-Down-Based Saliency Detection in a Traffic Driving Environment, Trans. ITS, 2016 | <a href=https://doi.org/10.1109/TITS.2016.2535402>paper</a> | <a href=https://github.com/taodeng/Top-down-based-traffic-driving-saliency-model>code</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@article{2016_T-ITS_Deng,
    author = "Deng, Tao and Yang, Kaifu and Li, Yongjie and Yan, Hongmei",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    number = "7",
    pages = "2051--2062",
    publisher = "IEEE",
    title = "Where does the driver look? Top-down-based saliency detection in a traffic driving environment",
    volume = "17",
    year = "2016"
}
</pre>
</ul>
</ul>
<ul><a name=2014_TransSysManCybernetics_Borji></a>
<details close>
<summary>Borji et al., What/Where to Look Next? Modeling Top-Down Visual Attention in Complex Interactive Environments, Transactions on Systems, Man, and Cybernetics Systems, 2014 | <a href=https://doi.org/10.1109/TSMC.2013.2279715>paper</a></summary>
<ul>
Dataset(s): <a href=datasets.md#3DDS>3DDS</a>
</ul>
<ul>
<pre>
@article{2014_TransSysManCybernetics_Borji,
    author = "Borji, Ali and Sihite, Dicky N and Itti, Laurent",
    journal = "IEEE Transactions on Systems, Man, and Cybernetics: Systems",
    number = "5",
    pages = "523--538",
    title = "{What/where to look next? Modeling top-down visual attention in complex interactive environments}",
    volume = "44",
    year = "2013"
}
</pre>
</ul>
</ul>
<ul><a name=2013_RSTB_Johnson></a>
<details close>
<summary>Johnson et al., Predicting human visuomotor behaviour in a driving task, Philosophical Transactions of the Royal Society: B, 2013 | <a href=https://doi.org/10.1098/rstb.2013.0044>paper</a> | <a href=https://github.com/EmbodiedCognition/driving-simulator>code</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@article{2013_RSTB_Johnson,
    author = "Johnson, Leif and Sullivan, Brian and Hayhoe, Mary and Ballard, Dana",
    journal = "Philosophical Transactions of the Royal Society B: Biological Sciences",
    number = "1636",
    pages = "20130044",
    title = "Predicting human visuomotor behaviour in a driving task",
    volume = "369",
    year = "2014"
}
</pre>
</ul>
</ul>
<ul><a name=2012_CVPR_Borji></a>
<details close>
<summary>Borji et al., Probabilistic Learning of Task-Specific Visual Attention, CVPR, 2012 | <a href=https://doi.org/10.1109/CVPR.2012.6247710>paper</a> | <a href=http://ilab.usc.edu/borji/Resources.html>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#3DDS>3DDS</a>
</ul>
<ul>
<pre>
@inproceedings{2012_CVPR_Borji,
    author = "Borji, Ali and Sihite, Dicky N and Itti, Laurent",
    booktitle = "CVPR",
    title = "Probabilistic learning of task-specific visual attention",
    year = "2012"
}
</pre>
</ul>
</ul>
<ul><a name=2011_BMVC_Borji></a>
<details close>
<summary>Borji et al., Computational Modeling of Top-down Visual Attention in Interactive Environments, BMVC, 2011 | <a href=http://www.bmva.org/bmvc/2011/proceedings/paper85/paper85.pdf>paper</a></summary>
<ul>
Dataset(s): <a href=datasets.md#3DDS>3DDS</a>
</ul>
<ul>
<pre>
@inproceedings{2011_BMVC_Borji,
    author = "Borji, Ali and Sihite, Dicky N and Itti, Laurent",
    booktitle = "BMVC",
    title = "Computational Modeling of Top-down Visual Attention in Interactive Environments.",
    year = "2011"
}
</pre>
</ul>
</ul>
