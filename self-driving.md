---
<a href=README.md/#top><l style="font-size:30px">Home</l></a>&nbsp;&nbsp;| <a href=behavioral.md><l style="font-size:30px">Behavioral</l></a>&nbsp;&nbsp;| <l style="font-size:35px">Applications</l>&nbsp;&nbsp;| <a href=datasets.md><l style="font-size:30px">Datasets</l></a>&nbsp;&nbsp;
---

[Scene gaze](scene_gaze.md)&nbsp;&nbsp;| [In-vehicle gaze](in-vehicle_gaze.md)&nbsp;&nbsp;| [Distraction detection](distraction_detection.md)&nbsp;&nbsp;| [Drowsiness detection](drowsiness_detection.md)&nbsp;&nbsp;| [Action anticipation](action_anticipation.md)&nbsp;&nbsp;| [Driver awareness](driver_awareness.md)&nbsp;&nbsp;| Self-driving&nbsp;&nbsp;| [Papers with code](papers_with_code.md)&nbsp;&nbsp;
___
*Click on each entry below to see additional information.*
<ul><a name=2023_T-ITS_Zhao></a>
<details close>
<summary>Zhao et al., Improving Autonomous Vehicle Visual Perception by Fusing Human Gaze and Machine Vision, Trans. ITS, 2023 | <a href=https://doi.org/10.1109/TITS.2023.3290016>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@article{2023_T-ITS_Zhao,
    author = "Zhao, Yiyue and Lei, Cailin and Shen, Yu and Du, Yuchuan and Chen, Qijun",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    publisher = "IEEE",
    title = "Improving Autonomous Vehicle Visual Perception by Fusing Human Gaze and Machine Vision",
    year = "2023"
}
</pre>
</ul>
</ul>
<ul><a name=2022_IV_Niu></a>
<details close>
<summary>Niu et al., Auditory and visual warning information generation of the risk object in driving scenes based on weakly supervised learning, IV, 2022 | <a href=https://doi.org/1109/IV51971.2022.9827382>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@inproceedings{2022_IV_Niu,
    author = "Niu, Yinjie and Ding, Ming and Zhang, Yuxiao and Ohtani, Kento and Takeda, Kazuya",
    booktitle = "2022 IEEE Intelligent Vehicles Symposium (IV)",
    organization = "IEEE",
    pages = "1572--1577",
    title = "Auditory and visual warning information generation of the risk object in driving scenes based on weakly supervised learning",
    year = "2022"
}
</pre>
</ul>
</ul>
<ul><a name=2022_IV_Li></a>
<details close>
<summary>Li et al., Enhancement of Target Feature Regions and Intention-Driven Visual Attention Selection in Traffic Scenes, IV, 2022 | <a href=https://doi.org/10.1109/IV51971.2022.9827298>paper</a></summary>
<ul>
Dataset(s): KITTI
</ul>
<ul>
<pre>
@inproceedings{2022_IV_Li,
    author = "Li, Jing and Zhang, Dongbo and Meng, Bumin and Chen, Renjie and Tang, Jiajun and Wang, Yaonan",
    booktitle = "2022 IEEE Intelligent Vehicles Symposium (IV)",
    organization = "IEEE",
    pages = "404--410",
    title = "Enhancement of Target Feature Regions and Intention-Driven Visual Attention Selection in Traffic Scenes",
    year = "2022"
}
</pre>
</ul>
</ul>
<ul><a name=2022_ICRA_Li></a>
<details close>
<summary>Li et al., Important Object Identification with Semi-Supervised Learning for Autonomous Driving, ICRA, 2022 | <a href=https://doi.org/10.1109/ICRA46639.2022.9812234>paper</a></summary>
<ul>
Dataset(s): <a href=datasets.md#H3D>H3D</a>
</ul>
<ul>
<pre>
@inproceedings{2022_ICRA_Li,
    author = "Li, Jiachen and Gang, Haiming and Ma, Hengbo and Tomizuka, Masayoshi and Choi, Chiho",
    booktitle = "2022 International Conference on Robotics and Automation (ICRA)",
    organization = "IEEE",
    pages = "2913--2919",
    title = "Important object identification with semi-supervised learning for autonomous driving",
    year = "2022"
}
</pre>
</ul>
</ul>
<ul><a name=2021_ICRA_Wei></a>
<details close>
<summary>Wei et al., Perceive, Attend, and Drive: Learning Spatial Attention for Safe Self-Driving, ICRA, 2021 | <a href=https://doi.org/10.1109/ICRA48506.2021.9561904>paper</a></summary>
<ul>
Dataset(s): Drive4D, nuScenes
</ul>
<ul>
<pre>
@inproceedings{2021_ICRA_Wei,
    author = "Wei, Bob and Ren, Mengye and Zeng, Wenyuan and Liang, Ming and Yang, Bin and Urtasun, Raquel",
    booktitle = "ICRA",
    title = "Perceive, Attend, and Drive: Learning Spatial Attention for Safe Self-Driving",
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2021_ICCV_Chitta></a>
<details close>
<summary>Chitta et al., NEAT: Neural Attention Fields for End-to-End Autonomous Driving, ICCV, 2021 | <a href=https://openaccess.thecvf.com/content/ICCV2021/papers/Chitta_NEAT_Neural_Attention_Fields_for_End-to-End_Autonomous_Driving_ICCV_2021_paper.pdf>paper</a> | <a href=https://github.com/autonomousvision/neat>code</a></summary>
<ul>
Dataset(s): CARLA
</ul>
<ul>
<pre>
@inproceedings{2021_ICCV_Chitta,
    author = "Chitta, Kashyap and Prakash, Aditya and Geiger, Andreas",
    booktitle = "ICCV",
    title = "NEAT: Neural Attention Fields for End-to-End Autonomous Driving",
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2021_CVPRW_Ishihara></a>
<details close>
<summary>Ishihara et al., Multi-task Learning with Attention for End-to-end Autonomous Driving, CVPRW, 2021 | <a href=https://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Ishihara_Multi-Task_Learning_With_Attention_for_End-to-End_Autonomous_Driving_CVPRW_2021_paper.pdf>paper</a></summary>
<ul>
Dataset(s): CARLA
</ul>
<ul>
<pre>
@inproceedings{2021_CVPRW_Ishihara,
    author = "Ishihara, Keishi and Kanervisto, Anssi and Miura, Jun and Hautamaki, Ville",
    booktitle = "CVPRW",
    title = "Multi-task Learning with Attention for End-to-end Autonomous Driving",
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2021_CVPR_Prakash></a>
<details close>
<summary>Prakash et al., Multi-Modal Fusion Transformer for End-to-End Autonomous Driving, CVPR, 2021 | <a href=https://openaccess.thecvf.com/content/CVPR2021/papers/Prakash_Multi-Modal_Fusion_Transformer_for_End-to-End_Autonomous_Driving_CVPR_2021_paper.pdf#page=1&zoom=auto,-100,798>paper</a> | <a href=https://github.com/autonomousvision/transfuser>code</a></summary>
<ul>
Dataset(s): CARLA
</ul>
<ul>
<pre>
@inproceedings{2021_CVPR_Prakash,
    author = "Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas",
    booktitle = "CVPR",
    title = "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving",
    year = "2021"
}
</pre>
</ul>
</ul>
<ul><a name=2020_WACV_Xia></a>
<details close>
<summary>Xia et al., Periphery-Fovea Multi-Resolution Driving Model Guided by Human Attention, WACV, 2020 | <a href=https://openaccess.thecvf.com/content_WACV_2020/papers/Xia_Periphery-Fovea_Multi-Resolution_Driving_Model_Guided_by_Human_Attention_WACV_2020_paper.pdf>paper</a> | <a href=https://github.com/pascalxia/periphery_fovea_driving>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-X>BDD-X</a>, <a href=datasets.md#BDD-A>BDD-A</a>, <a href=datasets.md#DR(eye)VE>DR(eye)VE</a>
</ul>
<ul>
<pre>
@inproceedings{2020_WACV_Xia,
    author = "Xia, Ye and Kim, Jinkyu and Canny, John and Zipser, Karl and Canas-Bajo, Teresa and Whitney, David",
    booktitle = "WACV",
    title = "Periphery-fovea multi-resolution driving model guided by human attention",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2020_IROS_Li_1></a>
<details close>
<summary>Li et al., End-to-end Contextual Perception and Prediction with Interaction Transformer, IROS, 2020 | <a href=https://doi.org/10.1109/IROS45743.2020.9341392>paper</a></summary>
<ul>
Dataset(s): ATG4D, nuScenes
</ul>
<ul>
<pre>
@inproceedings{2020_IROS_Li_1,
    author = "Li, Lingyun Luke and Yang, Bin and Liang, Ming and Zeng, Wenyuan and Ren, Mengye and Segal, Sean and Urtasun, Raquel",
    booktitle = "IROS",
    title = "End-to-end contextual perception and prediction with interaction transformer",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2020_ECCVW_Mittal></a>
<details close>
<summary>Mittal et al., AttnGrounder: Talking to Cars with Attention, ECCVW, 2020 | <a href=https://doi.org/10.1007/978-3-030-66096-3_6>paper</a> | <a href=https://github.com/i-m-vivek/AttnGrounder>code</a></summary>
<ul>
Dataset(s): Talk2Car
</ul>
<ul>
<pre>
@inproceedings{2020_ECCVW_Mittal,
    author = "Mittal, Vivek",
    booktitle = "ECCV",
    title = "Attngrounder: Talking to cars with attention",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2020_ECCV_Zhou></a>
<details close>
<summary>Zhou et al., DA4AD: End-to-end deep attention-based visual localization for autonomous driving, ECCV, 2020 | <a href=http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730273.pdf>paper</a></summary>
<ul>
Dataset(s): Apollo-DaoxiangLake
</ul>
<ul>
<pre>
@inproceedings{2020_ECCV_Zhou,
    author = "Zhou, Yao and Wan, Guowei and Hou, Shenhua and Yu, Li and Wang, Gang and Rui, Xiaofei and Song, Shiyu",
    booktitle = "ECCV",
    title = "DA4AD: End-to-end deep attention-based visual localization for autonomous driving",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2020_CVPRW_Kim></a>
<details close>
<summary>Kim et al., Attentional Bottleneck: Towards an Interpretable Deep Driving Network, CVPRW, 2020 | <a href=https://openaccess.thecvf.com/content_CVPRW_2020/papers/w20/Kim_Attentional_Bottleneck_Towards_an_Interpretable_Deep_Driving_Network_CVPRW_2020_paper.pdf>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@inproceedings{2020_CVPRW_Kim,
    author = "Kim, Jinkyu and Bansal, Mayank",
    booktitle = "CVPR",
    title = "Attentional bottleneck: Towards an interpretable deep driving network",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2020_CVPRW_Cultrera></a>
<details close>
<summary>Cultrera et al., Explaining Autonomous Driving by Learning End-to-End Visual Attention, CVPRW, 2020 | <a href=https://openaccess.thecvf.com/content_CVPRW_2020/papers/w20/Cultrera_Explaining_Autonomous_Driving_by_Learning_End-to-End_Visual_Attention_CVPRW_2020_paper.pdf>paper</a></summary>
<ul>
Dataset(s): CARLA
</ul>
<ul>
<pre>
@inproceedings{2020_CVPRW_Cultrera,
    author = "Cultrera, Luca and Seidenari, Lorenzo and Becattini, Federico and Pala, Pietro and Del Bimbo, Alberto",
    booktitle = "CVPRW",
    title = "{Explaining Autonomous Driving by Learning End-to-End Visual Attention}",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2020_CVPR_Kim></a>
<details close>
<summary>Kim et al., Advisable Learning for Self-driving Vehicles by Internalizing Observation-to-Action Rules, CVPR, 2020 | <a href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Advisable_Learning_for_Self-Driving_Vehicles_by_Internalizing_Observation-to-Action_Rules_CVPR_2020_paper.pdf>paper</a> | <a href=https://github.com/JinkyuKimUCB/advisable-driving>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-X>BDD-X</a>, CARLA
</ul>
<ul>
<pre>
@inproceedings{2020_CVPR_Kim,
    author = "Kim, Jinkyu and Moon, Suhong and Rohrbach, Anna and Darrell, Trevor and Canny, John",
    booktitle = "CVPR",
    title = "Advisable learning for self-driving vehicles by internalizing observation-to-action rules",
    year = "2020"
}
</pre>
</ul>
</ul>
<ul><a name=2019_IV_Mori></a>
<details close>
<summary>Mori et al., Visual Explanation by Attention Branch Network for End-to-end Learning-based Self-driving, IV, 2019 | <a href=https://doi.org/10.1109/IVS.2019.8813900>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@inproceedings{2019_IV_Mori,
    author = "Mori, Keisuke and Fukui, Hiroshi and Murase, Takuya and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu",
    booktitle = "IV",
    title = "Visual explanation by attention branch network for end-to-end learning-based self-driving",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2019_IROSW_Chen></a>
<details close>
<summary>Chen et al., Gaze Training by Modulated Dropout Improves Imitation Learning, IROSW, 2019 | <a href=https://doi.org/10.1109/IROS40897.2019.8967843>paper</a></summary>
<ul>
Dataset(s): TORCS
</ul>
<ul>
<pre>
@inproceedings{2019_IROSW_Chen,
    author = "Chen, Yuying and Liu, Congcong and Tai, Lei and Liu, Ming and Shi, Bertram E",
    booktitle = "IROS",
    title = "Gaze training by modulated dropout improves imitation learning",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2019_ICRA_Wang></a>
<details close>
<summary>Wang et al., Deep Object-Centric Policies for Autonomous Driving, ICRA, 2019 | <a href=https://doi.org/10.1109/ICRA.2019.8794224>paper</a></summary>
<ul>
Dataset(s): BDD
</ul>
<ul>
<pre>
@inproceedings{2019_ICRA_Wang,
    author = "Wang, Dequan and Devin, Coline and Cai, Qi-Zhi and Yu, Fisher and Darrell, Trevor",
    booktitle = "ICRA",
    title = "Deep object-centric policies for autonomous driving",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2019_ICCVW_Li></a>
<details close>
<summary>Li et al., DBUS: Human Driving Behavior Understanding System, ICCVW, 2019 | <a href=https://openaccess.thecvf.com/content_ICCVW_2019/papers/ADW/Li_DBUS_Human_Driving_Behavior_Understanding_System_ICCVW_2019_paper.pdf>paper</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@inproceedings{2019_ICCVW_Li,
    author = "Li, Max Guangyu and Jiang, Bo and Che, Zhengping and Shi, Xuefeng and Liu, Mengyao and Meng, Yiping and Ye, Jieping and Liu, Yan",
    booktitle = "ICCVW",
    title = "DBUS: Human Driving Behavior Understanding System.",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2019_CVPR_Kim></a>
<details close>
<summary>Kim et al., Grounding Human-to-Vehicle Advice for Self-driving Vehicles, CVPR, 2019 | <a href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Grounding_Human-To-Vehicle_Advice_for_Self-Driving_Vehicles_CVPR_2019_paper.pdf>paper</a></summary>
<ul>
Dataset(s): <a href=datasets.md#HAD>HAD</a>
</ul>
<ul>
<pre>
@inproceedings{2019_CVPR_Kim,
    author = "Kim, Jinkyu and Misu, Teruhisa and Chen, Yi-Ting and Tawari, Ashish and Canny, John",
    booktitle = "CVPR",
    title = "Grounding human-to-vehicle advice for self-driving vehicles",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2019_ACM_Liu></a>
<details close>
<summary>Liu et al., A Gaze Model Improves Autonomous Driving, ACM Symposium on Eye Tracking Research and Applications, 2019 | <a href=https://doi.org/10.1145/3314111.3319846>paper</a></summary>
<ul>
Dataset(s): TORCS
</ul>
<ul>
<pre>
@inproceedings{2019_ACM_Liu,
    author = "Liu, Congcong and Chen, Yuying and Tai, Lei and Ye, Haoyang and Liu, Ming and Shi, Bertram E",
    booktitle = "ETRA",
    title = "A gaze model improves autonomous driving",
    year = "2019"
}
</pre>
</ul>
</ul>
<ul><a name=2018_ITSC_Mund></a>
<details close>
<summary>Mund et al., Visualizing the Learning Progress of Self-Driving Cars, ITSC, 2018 | <a href=https://doi.org/10.1109/ITSC.2018.8569464>paper</a></summary>
<ul>
Dataset(s): <a href=datasets.md#Dashcam dataset>Dashcam dataset</a>
</ul>
<ul>
<pre>
@inproceedings{2018_ITSC_Mund,
    author = {Mund, Sandro and Frank, Rapha{\"e}l and Varisteas, Georgios and State, Radu},
    booktitle = "ITSC",
    title = "{Visualizing the Learning Progress of Self-Driving Cars}",
    year = "2018"
}
</pre>
</ul>
</ul>
<ul><a name=2018_ICPR_He></a>
<details close>
<summary>He et al., Aggregated Sparse Attention for Steering Angle Prediction, ICPR, 2018 | <a href=https://doi.org/10.1109/ICPR.2018.8546051>paper</a></summary>
<ul>
Dataset(s): DIPLECS, Comma.ai
</ul>
<ul>
<pre>
@inproceedings{2018_ICPR_He,
    author = "He, Sen and Kangin, Dmitry and Mi, Yang and Pugeault, Nicolas",
    booktitle = "ICPR",
    title = "{Aggregated Sparse Attention for Steering Angle Prediction}",
    year = "2018"
}
</pre>
</ul>
</ul>
<ul><a name=2018_ECCV_Kim></a>
<details close>
<summary>Kim et al., Textual Explanations for Self-Driving Vehicles, ECCV, 2018 | <a href=https://openaccess.thecvf.com/content_ECCV_2018/papers/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.pdf>paper</a> | <a href=https://github.com/JinkyuKimUCB/explainable-deep-driving>code</a></summary>
<ul>
Dataset(s): <a href=datasets.md#BDD-X>BDD-X</a>
</ul>
<ul>
<pre>
@inproceedings{2018_ECCV_Kim,
    author = "Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep",
    booktitle = "ECCV",
    title = "Textual explanations for self-driving vehicles",
    year = "2018"
}
</pre>
</ul>
</ul>
<ul><a name=2017_ICCV_Kim></a>
<details close>
<summary>Kim et al., Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention, ICCV, 2017 | <a href=https://openaccess.thecvf.com/content_ICCV_2017/papers/Kim_Interpretable_Learning_for_ICCV_2017_paper.pdf>paper</a></summary>
<ul>
Dataset(s): Comma.ai, Udacity, private
</ul>
<ul>
<pre>
@inproceedings{2017_ICCV_Kim,
    author = "Kim, Jinkyu and Canny, John",
    booktitle = "ICCV",
    title = "Interpretable learning for self-driving cars by visualizing causal attention",
    year = "2017"
}
</pre>
</ul>
</ul>
<ul><a name=2017_arXiv_Bojarski></a>
<details close>
<summary>Bojarski et al., Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car, arXiv, 2017 | <a href=https://arxiv.org/pdf/1704.07911.pdf>paper</a> | <a href=https://github.com/AutoDeep/PilotNet>code</a></summary>
<ul>
Dataset(s): private
</ul>
<ul>
<pre>
@article{2017_arXiv_Bojarski,
    author = "Bojarski, Mariusz and Yeres, Philip and Choromanska, Anna and Choromanski, Krzysztof and Firner, Bernhard and Jackel, Lawrence and Muller, Urs",
    journal = "arXiv:1704.07911",
    title = "Explaining how a deep neural network trained with end-to-end learning steers a car",
    year = "2017"
}
</pre>
</ul>
</ul>
